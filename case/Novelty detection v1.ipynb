{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import twitter_samples\n",
    "import gensim\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier, KDTree\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения исследования, скачаем весь датасет, и будем собирать батчи из него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_dataset_4_9000_2000.csv', low_memory=False)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generator(batch_size=10):\n",
    "    \n",
    "    # df = pd.read_csv('twitter_dataset.csv')\n",
    "    for i in range(0, df.shape[0], batch_size):\n",
    "        stream_batch = df.iloc[i : min(i + batch_size, df.shape[0])]\n",
    "        yield stream_batch['content'].tolist(), stream_batch['novel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Tune in 11:30 ET tomorrow for a live webcast of Families USA Presidential Forum on health care: http://presidentialforums.health08.org/', 'PHOTOS: Tour with Sen. Mark Udall of Ascent Solar Technologies in Thornton. http://tinyurl.com/ko4egc', 'Michael was out early on Sunday giving doughnuts to participants in the Denver PrideFest Parade: http://tinyurl.com/lm876m. PHOTOS.', 'BLOG: Michael was recently interviewed about education. He urged the system must be adapted to meet 21st century needs. http://bit.ly/1avORn', 'BLOG: Bennet votes against $1.75 billion in government waste; supports elimination of more spending on F-22. Read more: http://bit.ly/b9B7Z', \"Thanks for the support @kaffeinebuzz.  I'm committed to responsible reforms that will lower cost and increase coverage for Coloradans.\", 'PHOTOS: Michael attended the annual summer picnic hosted by the Democratic Party of Denver on Sunday, August 2. http://bit.ly/4zueoq', \"LINK: Dealers applaud 'Clunkers' extension, The Coloradoan. http://bit.ly/LjBh6\", 'LINK: Bennet talks transit, health care during Aspen stop. Aspen Times. http://bit.ly/rCKMs.', 'Congratulations to Ronile and Richard -- the winners of our raffle for tickets to see President Obama and Michael Bennet tomorrow.'], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "for batch in stream_generator():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем удалять по возможности все теги(@) и ссылки. Некоторые стоп-слова также удалим. Применим в одном варианте лемматизацию, в другом - стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.methods = {\n",
    "            'lemm' : self.lemmatization,\n",
    "            'stem' : self.stemming\n",
    "        }\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.swords = set( stopwords.words(\"english\") )\n",
    "     \n",
    "    def preprocess1(self, content_batch, standard='lemm'):\n",
    "        # stream_batch = ([content], [novel]), content = list(string), novel = list\n",
    "    \n",
    "        preprocessed_batch = []\n",
    "        for doc in content_batch:\n",
    "            doc = doc.lower()\n",
    "            doc = self.delete_tags(doc)\n",
    "            doc = self.delete_links(doc)\n",
    "            doc = self.delete_garbage(doc)\n",
    "            tokens = self.get_tokens(doc)\n",
    "            tokens = self.methods[standard](tokens)\n",
    "            tokens = self.delete_stop_words(tokens)\n",
    "            preprocessed_batch.append( ' '.join(tokens) )\n",
    "            \n",
    "        return preprocessed_batch\n",
    "    \n",
    "    \n",
    "    def delete_tags(self, doc):\n",
    "        doc = re.sub(r'^@[\\w]*', ' ', doc) \n",
    "        doc = re.sub(r'\\s@[\\w]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_links(self, doc):\n",
    "        doc = re.sub(r'http\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'https\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'ftp\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'www\\.[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_garbage(self, doc):\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "        doc = re.sub(r\"[^a-zA-Z0-9\\s\\']*\", '', doc) # TODO: посмотреть что оставляет CountVectorizer\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_stop_words(self, tokens): # TODO: create own sw list\n",
    "        # new_tokens = []\n",
    "        return list( filter(lambda sword: sword not in self.swords, tokens) )\n",
    "#         for sword in swords:\n",
    "#             if sword not in stop_words:\n",
    "#                 new_tokens.append(sword)\n",
    "#         return new_tokens\n",
    "    \n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        return list(map(lambda token: token.lower(), doc.split()))\n",
    "    \n",
    "    \n",
    "    def lemmatization(self, tokens):\n",
    "        return list(map(lambda token: self.lemmatizer.lemmatize(token), tokens))\n",
    "    \n",
    "    \n",
    "    def stemming(self, tokens):\n",
    "        return list( map(lambda token: self.stemmer.stem(token), tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем твиттер-корпус, и создадим из него фиксированный словарь для векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    twitter_samples.fileids()\n",
    "    vocab_corpus = ([(t, \"pos\") for t in twitter_samples.strings(\"positive_tweets.json\")] + \n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"negative_tweets.json\")] +\n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"tweets.20150430-223406.json\")]\n",
    "            )\n",
    "    vocab_corpus = list( map( lambda pair: pair[0], vocab_corpus ) )\n",
    "    return vocab_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_corpus = create_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print( len(vocab_corpus) )\n",
    "print( vocab_corpus[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, в котором будут реализованы основные методы векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self, method, train_corpus):\n",
    "        self.methods = {\n",
    "            'one-hot' : self.one_hot_vectorizer,\n",
    "            'count' : self.count_vectorizer,\n",
    "            'tf-idf' : self.tfidf_vectorizer,\n",
    "            #'n-gramms' : self.n_gramms_vectorizer,\n",
    "            'doc-to-vec' : self.doc_to_vec_vectorizer\n",
    "        }\n",
    "        if method not in self.methods:\n",
    "            raise Exception('Wrong method: {}'.format(method))\n",
    "        \n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.train_corpus = Preprocessor().preprocess1( train_corpus )\n",
    "        \n",
    "    \n",
    "    def vectorize(self, batch, **args):\n",
    "        return self.methods[self.method](batch, **args)\n",
    "        \n",
    "    \n",
    "    def one_hot_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(binary=True, **args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def count_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def tfidf_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = TfidfVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "        \n",
    "        \n",
    "    def doc_to_vec_vectorizer(self, batch, **args):\n",
    "        \n",
    "        def extract_tokens(train = False):\n",
    "            if train:\n",
    "                for i, doc in enumerate(self.train_corpus):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])    \n",
    "                    \n",
    "            else:\n",
    "                for i, doc in enumerate(batch):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield tokens\n",
    "\n",
    "        \n",
    "        if self.model is None:\n",
    "            # vocab = list( extract_tokens(train=True) )\n",
    "            vocab = list( extract_tokens(train=True) )\n",
    "            self.model = gensim.models.doc2vec.Doc2Vec(min_count=1, vector_size=20)\n",
    "            # self.model.build_vocab(train_corpus, update = True)\n",
    "            # self.model.build_vocab(self.train_corpus)\n",
    "            self.model.build_vocab( vocab )\n",
    "            self.model.train(vocab, total_examples=self.model.corpus_count, epochs=5, **args)\n",
    "        \n",
    "        return np.array( list( map( lambda token: self.model.infer_vector(token), extract_tokens() ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализируем на плоскости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = Vectorizer('tf-idf', vocab_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 14999 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 14999 samples in 31.460s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 14999\n",
      "[t-SNE] Computed conditional probabilities for sample 14999 / 14999\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 88.921951\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.764202\n"
     ]
    }
   ],
   "source": [
    "X = df['content'].tolist()\n",
    "X = pp.preprocess1(X)\n",
    "X = v1.vectorize(X)\n",
    "Xe = TSNE(n_components=2, n_iter=n_iter, n_jobs=n_jobs, verbose=1).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (df['type'] == 1).astype(dtype=int).to_numpy()\n",
    "mask2 = (df['type'] == 2).astype(dtype=int).to_numpy()\n",
    "mask3 = (df['type'] == 3).astype(dtype=int).to_numpy()\n",
    "mask4 = (df['type'] == 4).astype(dtype=int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = Xe[np.where(mask1)]\n",
    "X2 = Xe[np.where(mask2)]\n",
    "X3 = Xe[np.where(mask3)]\n",
    "X4 = Xe[np.where(mask4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f83bd7f5310>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X1[:, 0], X1[:, 1])\n",
    "plt.scatter(X2[:, 0], X2[:, 1])\n",
    "plt.scatter(X3[:, 0], X3[:, 1])\n",
    "plt.scatter(X4[:, 0], X4[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной визуалиции кластеры разных тем никак не отделены друг от друга. Посмотрим, как с этим справятся алгоритмы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве обратной меры новизны будем использовать отношение средней плотности k ближайших к объекту соседей и локальной плотности самого объекта. Если это отношение будет достаточно близко к нулю, то можно воспринимать объект как аномальный.\n",
    "\n",
    "Локальную плотность считаем так - находим минимальный радиус сферы, внутри которой находятся k ближайших соседей для данного объекта(считая сам объект).\n",
    "\n",
    "Но нет, лучше зафиксируем k, и будем смотреть r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "\n",
    "- Считываем достаточное количество примеров из потока для обучения: n_start = 200. \n",
    "- Далее считываем по батчу данных размера n_batch\n",
    "- Для каждого объекта в батче считаем локальную плотность, и заодно ищем индексы k_nbr = 5 ближайших соседей \n",
    "- Для каждого из k_nbr ближайших соседей считаем локальную плотность\n",
    "- Усредняем, делим, получаем обратную меру новизны\n",
    "- Сравниваем с порогом(который сначала нужно подобрать)\n",
    "- В конце обработки элемента добавляем его в известную выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ро = k / r^dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knn(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, n_neighbors=5, max_samples = 5000,\n",
    "                 default_normal_score=1, eps=0.5683, algorithm='kd_tree', metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        self.metric = metric \n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = default_normal_score\n",
    "        self.threshold = eps\n",
    "        \n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype) \n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "        \n",
    "        for x in X[start_count: ]:\n",
    "            score = self.get_novel_score(x)\n",
    "            if self.pos >= self.data.shape[0]:\n",
    "                self.data = np.append(self.data, [x], axis=0)\n",
    "            self.pos = (self.pos + 1) % self.max_samples\n",
    "            yield score\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_novel_score(self, obj): # normal score\n",
    "\n",
    "        dist, ind = self.get_neighbors([obj])\n",
    "        int_lof = dist[0, -1] # radius of sphere\n",
    "        \n",
    "        dist, ind = self.get_neighbors( [self.data[i] for i in ind[0]] )\n",
    "        ext_lof = np.mean( dist[:, -1] )\n",
    "        \n",
    "        return (1 + ext_lof) / (1 + int_lof)\n",
    "    \n",
    "    \n",
    "    def get_neighbors(self, X):\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.algorithm == 'kd_tree':\n",
    "\n",
    "            tree = KDTree(self.data)\n",
    "            dist, ind = tree.query(X, k=self.n_neighbors)\n",
    "            return dist, ind\n",
    "\n",
    "        else:\n",
    "            raise Exception('unknown algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alg(pp, alg, svd_compenents, n_iter, batch_size, vectorization, eps):\n",
    "\n",
    "    svd = TruncatedSVD(n_components=svd_compenents, n_iter=n_iter, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    answers = []\n",
    "    targets = []\n",
    "\n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "        vectors = v1.vectorize(batch)\n",
    "        if vectorization != 'doc-to-vec':\n",
    "            vectors = svd.fit_transform(vectors)\n",
    "\n",
    "        scores.append( list(alg.check_novelty(vectors)) )\n",
    "        \n",
    "        answers += [int(score < alg.threshold) for score in scores[-1]]\n",
    "        targets += target\n",
    "\n",
    "        scores.append( list(alg.check_novelty(vectors)) )\n",
    "    \n",
    "    print( svd_compenents, n_iter, batch_size, vectorization, eps)\n",
    "    print( accuracy_score(answers, targets) )\n",
    "    print( f1_score(answers, targets) )\n",
    "    print( roc_auc_score(answers, targets) )\n",
    "    print()\n",
    "    \n",
    "    return accuracy_score(answers, targets), f1_score(answers, targets), roc_auc_score(answers, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбираем параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты замеров есть в таблице:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1a6M76eS8L-WTGKVI-xRajut2P-X70j-bv5s_7WZv3ZE/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.68888, 0.76666, 0.8444444444444446]:\n",
    "    try:\n",
    "        alg = Knn(max_samples=2000, eps=eps)\n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=5, n_iter=7,\n",
    "                                   batch_size=300, vectorization='tf-idf', eps=eps)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    m_acc.append(acc)\n",
    "    m_f1.append(f1)\n",
    "    m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "alg = Knn(max_samples=2000, )\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.55, 0.68888, 0.76666, 0.8444444444444446]:\n",
    "    for batch_size in [2000, 500, 300]:\n",
    "        for n_iter in [10, 25, 50]:\n",
    "            try:\n",
    "                alg = Knn(max_samples=2000, eps=eps)\n",
    "                acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=5, n_iter=n_iter,\n",
    "                                           batch_size=batch_size, vectorization='tf-idf', eps=eps)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            m_acc.append(acc)\n",
    "            m_f1.append(f1)\n",
    "            m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "alg = Knn(max_samples=2000, )\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.68888, 0.76666, 0.8444444444444446]:\n",
    "    for batch_size in [500, 300]:\n",
    "        for svd_compenents in [3, 10, 20, 30]:\n",
    "            try:\n",
    "                alg = Knn(max_samples=2000, eps=eps)\n",
    "                acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=svd_compenents, n_iter=10,\n",
    "                                           batch_size=batch_size, vectorization='tf-idf', eps=eps)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            m_acc.append(acc)\n",
    "            m_f1.append(f1)\n",
    "            m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be981928a5142a2a2157483a3075dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 10 500 doc-to-vec 0.68888\n",
      "0.979865324354957\n",
      "0.0\n",
      "0.48999799959991996\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198a104927a046758774ffbc396ad508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-61efaf7d2104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvectorization\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'doc-to-vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0malg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=20, n_iter=10,\n\u001b[0m\u001b[1;32m     19\u001b[0m                                            batch_size=batch_size, vectorization=vectorization, eps=eps)\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e31c31e5979>\u001b[0m in \u001b[0;36mrun_alg\u001b[0;34m(pp, alg, svd_compenents, n_iter, batch_size, vectorization, eps)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvectorization\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'doc-to-vec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-858942d83bc9>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, batch, **args)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-858942d83bc9>\u001b[0m in \u001b[0;36mdoc_to_vec_vectorizer\u001b[0;34m(self, batch, **args)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# self.model.build_vocab(self.train_corpus)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_doctags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         super(Doc2Vec, self).train(\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return super(BaseWordEmbeddingsModel, self).train(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0mdata_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             report_delay=report_delay, is_corpus_file_mode=False)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "alg = Knn(max_samples=2000, )\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.68888, 0.76666]:\n",
    "    for batch_size in [500]:\n",
    "        for vectorization in ['doc-to-vec']:\n",
    "            alg = Knn(max_samples=2000, eps=eps)\n",
    "            acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=20, n_iter=10,\n",
    "                                           batch_size=batch_size, vectorization=vectorization, eps=eps)\n",
    "            try:\n",
    "                pass\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            m_acc.append(acc)\n",
    "            m_f1.append(f1)\n",
    "            m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть предположение, что аномалии в нашем датасете не отличимы от нормальных данных. Проверим это, перебрав пороги по всем аномальным твитам и анализируя поведение метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alg_with_scores(pp, alg, n_iter, batch_size, vectorization, eps, svd_compenents=10):\n",
    "\n",
    "    svd = TruncatedSVD(n_components=svd_compenents, n_iter=n_iter, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    answers = []\n",
    "    targets = []\n",
    "\n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "        vectors = v1.vectorize(batch)\n",
    "        if vectorization != 'doc-to-vec':\n",
    "            vectors = svd.fit_transform(vectors)\n",
    "\n",
    "        scores.append( list(alg.check_novelty(vectors)) )\n",
    "        \n",
    "        answers += [int(score < alg.threshold) for score in scores[-1]]\n",
    "        targets += target\n",
    "    \n",
    "    print( svd_compenents, n_iter, batch_size, vectorization, eps)\n",
    "    print( accuracy_score(answers, targets) )\n",
    "    print( f1_score(answers, targets) )\n",
    "    print( roc_auc_score(answers, targets) )\n",
    "    print()\n",
    "    \n",
    "    return accuracy_score(answers, targets), f1_score(answers, targets), roc_auc_score(answers, targets), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bb35dadc114d09ac6a0e7b6f6defdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 10 500 doc-to-vec 0.8\n",
      "0.9795986399093273\n",
      "0.0\n",
      "0.4899953311545388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc, f1, roc_auc, scores = run_alg_with_scores(pp, alg, n_iter=10, batch_size=500,\n",
    "                                               vectorization='doc-to-vec', eps=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b776ae6ced41a99ec30c3d407ddd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for batch, target in tqdm( stream_generator(500) ):\n",
    "    targets += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = []\n",
    "for lst in scores:\n",
    "    sr += lst\n",
    "sr = np.array(sr).ravel()\n",
    "\n",
    "tr = np.array(targets).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = np.sort(sr[np.where(tr == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9781985465697713\n",
      "0.0\n",
      "0.4899812984237243\n",
      "\n",
      "0.9641309420628041\n",
      "0.003703703703703704\n",
      "0.4919539207715066\n",
      "\n",
      "0.9491299419961331\n",
      "0.0052151238591916565\n",
      "0.4918880933814528\n",
      "\n",
      "0.9483965597706514\n",
      "0.007692307692307692\n",
      "0.49289702286658865\n",
      "\n",
      "0.9354623641576105\n",
      "0.00819672131147541\n",
      "0.4926255490920178\n",
      "\n",
      "0.8918594572971531\n",
      "0.006127450980392158\n",
      "0.4910844571797963\n",
      "\n",
      "0.8763917594506301\n",
      "0.006430868167202571\n",
      "0.49097250922914437\n",
      "\n",
      "0.8756583772251484\n",
      "0.00745077168706759\n",
      "0.4913000503063211\n",
      "\n",
      "0.8751916794452963\n",
      "0.00847457627118644\n",
      "0.49163230604900254\n",
      "\n",
      "0.8678578571904794\n",
      "0.009\n",
      "0.49170638659253463\n",
      "\n",
      "0.8643909593972932\n",
      "0.00973709834469328\n",
      "0.4919030997691519\n",
      "\n",
      "0.8599239949329955\n",
      "0.010362694300518135\n",
      "0.49205009540443384\n",
      "\n",
      "0.8591239415961064\n",
      "0.011230697239120263\n",
      "0.49232560834596084\n",
      "\n",
      "0.8559903993599574\n",
      "0.011893870082342179\n",
      "0.49250310883640597\n",
      "\n",
      "0.8531902126808454\n",
      "0.012556053811659194\n",
      "0.4926850193690959\n",
      "\n",
      "0.8520568037869192\n",
      "0.013339261894175189\n",
      "0.49292858701485587\n",
      "\n",
      "0.837989199279952\n",
      "0.012997562956945572\n",
      "0.4926385029628955\n",
      "\n",
      "0.8349223281552104\n",
      "0.013545816733067729\n",
      "0.492781958052894\n",
      "\n",
      "0.8349223281552104\n",
      "0.014331210191082803\n",
      "0.4930418919757875\n",
      "\n",
      "0.8258550570038002\n",
      "0.014339622641509432\n",
      "0.4929349557529567\n",
      "\n",
      "0.8237215814387626\n",
      "0.014903129657228018\n",
      "0.4930967315462842\n",
      "\n",
      "0.8180545369691313\n",
      "0.015156983038614222\n",
      "0.4931142343134382\n",
      "\n",
      "0.8150543369557971\n",
      "0.015613910574875798\n",
      "0.4932316183346901\n",
      "\n",
      "0.8135875725048336\n",
      "0.016185784658691065\n",
      "0.4934057500839868\n",
      "\n",
      "0.8088539235949064\n",
      "0.016466552315608923\n",
      "0.49344549928113\n",
      "\n",
      "0.804120274684979\n",
      "0.01673360107095047\n",
      "0.49348142425376457\n",
      "\n",
      "0.8035202346823122\n",
      "0.017339113037679228\n",
      "0.4936783873581604\n",
      "\n",
      "0.802720181345423\n",
      "0.017922336541652838\n",
      "0.49386583454333477\n",
      "\n",
      "0.7826521768117874\n",
      "0.016887816646562123\n",
      "0.49329249814779597\n",
      "\n",
      "0.7819187945863058\n",
      "0.01742264944427756\n",
      "0.4934670918520026\n",
      "\n",
      "0.7804520301353424\n",
      "0.01789442290486132\n",
      "0.4936123462887284\n",
      "\n",
      "0.7767851190079339\n",
      "0.01818181818181818\n",
      "0.49367094444620246\n",
      "\n",
      "0.7721181412094139\n",
      "0.018380241240666284\n",
      "0.4936886886000402\n",
      "\n",
      "0.7699179945329688\n",
      "0.018765993744668752\n",
      "0.4937981585161203\n",
      "\n",
      "0.7672511500766718\n",
      "0.019106490587243608\n",
      "0.49388753356580883\n",
      "\n",
      "0.7641842789519301\n",
      "0.019406709176601054\n",
      "0.4939592701948541\n",
      "\n",
      "0.7549169944662978\n",
      "0.019210245464247596\n",
      "0.49379283560588055\n",
      "\n",
      "0.7525168344556303\n",
      "0.01954569466455362\n",
      "0.4938850711254144\n",
      "\n",
      "0.7511834122274819\n",
      "0.019957983193277313\n",
      "0.494015965298111\n",
      "\n",
      "0.7421161410760717\n",
      "0.019766852508869743\n",
      "0.4938535678391134\n",
      "\n",
      "0.7317821188079205\n",
      "0.01949792834511333\n",
      "0.49364771630151905\n",
      "\n",
      "0.7273151543436229\n",
      "0.019654841802492804\n",
      "0.49365606433832676\n",
      "\n",
      "0.7273151543436229\n",
      "0.02012458073790129\n",
      "0.4938252481249239\n",
      "\n",
      "0.7167811187412494\n",
      "0.019843101061375174\n",
      "0.4936105913234995\n",
      "\n",
      "0.7043136209080605\n",
      "0.019456113199204067\n",
      "0.49333131870034724\n",
      "\n",
      "0.6897793186212414\n",
      "0.018975332068311195\n",
      "0.4929857070448237\n",
      "\n",
      "0.6850456697113141\n",
      "0.01910299003322259\n",
      "0.49297815003869355\n",
      "\n",
      "0.6726448429895326\n",
      "0.018784972022382093\n",
      "0.4927082300865951\n",
      "\n",
      "0.6639775985065671\n",
      "0.018691588785046728\n",
      "0.49256486515455644\n",
      "\n",
      "0.6637109140609374\n",
      "0.019058732010890703\n",
      "0.4927038819733\n",
      "\n",
      "0.6443762917527835\n",
      "0.018402649981597352\n",
      "0.49219843816730385\n",
      "\n",
      "0.6409760650710047\n",
      "0.01858939311098961\n",
      "0.49222759185586656\n",
      "\n",
      "0.6342422828188546\n",
      "0.018604651162790697\n",
      "0.4921432786721186\n",
      "\n",
      "0.6322421494766318\n",
      "0.018854500177872643\n",
      "0.492217111382355\n",
      "\n",
      "0.6289085939062604\n",
      "0.019034191046880505\n",
      "0.4922449697678577\n",
      "\n",
      "0.6280418694579639\n",
      "0.019335559852346636\n",
      "0.4923558491697641\n",
      "\n",
      "0.6278418561237415\n",
      "0.01966982788900597\n",
      "0.4924892350553577\n",
      "\n",
      "0.6261084072271484\n",
      "0.019923103809856693\n",
      "0.4925697178785073\n",
      "\n",
      "0.6223081538769252\n",
      "0.020065732572219337\n",
      "0.4925782505105893\n",
      "\n",
      "0.6211747449829988\n",
      "0.0203448275862069\n",
      "0.4926780905167051\n",
      "\n",
      "0.6204413627575172\n",
      "0.0206433855152245\n",
      "0.4927914619151582\n",
      "\n",
      "0.614640976065071\n",
      "0.020670958996950185\n",
      "0.4927276595943535\n",
      "\n",
      "0.6021068071204747\n",
      "0.020354563361785948\n",
      "0.492427261535932\n",
      "\n",
      "0.59497299819988\n",
      "0.020319303338171262\n",
      "0.49231323766750196\n",
      "\n",
      "0.5785052336822455\n",
      "0.01984496124031008\n",
      "0.4918684119375944\n",
      "\n",
      "0.5751716781118741\n",
      "0.019993848046754848\n",
      "0.4918834188914847\n",
      "\n",
      "0.5733048869924662\n",
      "0.020208205756276788\n",
      "0.4919497501498281\n",
      "\n",
      "0.5726381758783919\n",
      "0.02047677261613692\n",
      "0.4920584686484633\n",
      "\n",
      "0.5709047269817988\n",
      "0.020693852708460133\n",
      "0.4921288363231164\n",
      "\n",
      "0.5672378158543903\n",
      "0.0208176195504601\n",
      "0.4921292988118077\n",
      "\n",
      "0.5662377491832788\n",
      "0.021065302437556426\n",
      "0.49222509684506127\n",
      "\n",
      "0.5613707580505367\n",
      "0.021127808361850912\n",
      "0.49218063405191975\n",
      "\n",
      "0.5610374024934995\n",
      "0.02140309155766944\n",
      "0.492299832788777\n",
      "\n",
      "0.5591706113740916\n",
      "0.021604024859425863\n",
      "0.4923630625584663\n",
      "\n",
      "0.5581038735915728\n",
      "0.021841794569067298\n",
      "0.49245516093248237\n",
      "\n",
      "0.5565037669177946\n",
      "0.022052337547780066\n",
      "0.49252748912014\n",
      "\n",
      "0.5557037135809054\n",
      "0.02230046948356807\n",
      "0.49262892230856115\n",
      "\n",
      "0.552836855790386\n",
      "0.022445707622795512\n",
      "0.4926539761645403\n",
      "\n",
      "0.5465031002066805\n",
      "0.022420235699913766\n",
      "0.4925498580356838\n",
      "\n",
      "0.5429028601906793\n",
      "0.02252637581978899\n",
      "0.4925458724048993\n",
      "\n",
      "0.5429028601906793\n",
      "0.02280501710376283\n",
      "0.492675713663116\n",
      "\n",
      "0.5387025801720114\n",
      "0.02287812455867815\n",
      "0.4926481605852666\n",
      "\n",
      "0.535502366824455\n",
      "0.02299817697377647\n",
      "0.4926572894411331\n",
      "\n",
      "0.5314354290286019\n",
      "0.023074784542674448\n",
      "0.4926329290264295\n",
      "\n",
      "0.5264350956730449\n",
      "0.023105487553293908\n",
      "0.4925720457526473\n",
      "\n",
      "0.5245683045536369\n",
      "0.023284481577866042\n",
      "0.49262970113332505\n",
      "\n",
      "0.5197013134208948\n",
      "0.023318872017353578\n",
      "0.4925718941076944\n",
      "\n",
      "0.5179011934128942\n",
      "0.023497636731937877\n",
      "0.49263118979540127\n",
      "\n",
      "0.5159010600706714\n",
      "0.023665456501277396\n",
      "0.4926824809368358\n",
      "\n",
      "0.5147676511767452\n",
      "0.023873390557939914\n",
      "0.492767072452555\n",
      "\n",
      "0.5147009800653377\n",
      "0.02413192116905751\n",
      "0.4928929369624648\n",
      "\n",
      "0.5027668511234082\n",
      "0.02382198952879581\n",
      "0.4925548312202737\n",
      "\n",
      "0.49863324221614774\n",
      "0.023883696780893044\n",
      "0.49251963869146764\n",
      "\n",
      "0.49716647776518436\n",
      "0.02406832298136646\n",
      "0.49258951180307303\n",
      "\n",
      "0.4955663710914061\n",
      "0.024245550683518184\n",
      "0.4926539146304536\n",
      "\n",
      "0.49343289552636843\n",
      "0.024396507447354904\n",
      "0.4926968313282968\n",
      "\n",
      "0.4932995533035536\n",
      "0.02464065708418891\n",
      "0.49281951545348923\n",
      "\n",
      "0.48443229548636574\n",
      "0.024473319036205372\n",
      "0.49259067527256495\n",
      "\n",
      "0.4810987399159944\n",
      "0.02456448176463216\n",
      "0.49258322499016194\n",
      "\n",
      "0.4806987132475498\n",
      "0.024790284211844246\n",
      "0.4926949136753375\n",
      "\n",
      "0.47869857990532705\n",
      "0.024940765681506424\n",
      "0.4927412592589429\n",
      "\n",
      "0.47803186879125276\n",
      "0.025152533931017306\n",
      "0.4928419947225171\n",
      "\n",
      "0.4758317221148077\n",
      "0.025291346392263825\n",
      "0.4928798206093135\n",
      "\n",
      "0.47389825988399226\n",
      "0.0254415215511918\n",
      "0.4929283480708424\n",
      "\n",
      "0.4680978731915461\n",
      "0.025409235279745906\n",
      "0.49281637365924286\n",
      "\n",
      "0.4650310020668045\n",
      "0.025504007772649988\n",
      "0.49281666637230614\n",
      "\n",
      "0.4648976598439896\n",
      "0.025734401553775186\n",
      "0.49293931381319517\n",
      "\n",
      "0.4602306820454697\n",
      "0.025752105896510228\n",
      "0.4928717394142999\n",
      "\n",
      "0.45989732648843257\n",
      "0.025970902969820846\n",
      "0.4929860447294272\n",
      "\n",
      "0.45989732648843257\n",
      "0.026205072725087153\n",
      "0.4931144102241417\n",
      "\n",
      "0.4560304020268018\n",
      "0.026256116481680393\n",
      "0.49307944953984034\n",
      "\n",
      "0.4538969264617641\n",
      "0.026387733269939376\n",
      "0.49311737572315295\n",
      "\n",
      "0.45096339755983733\n",
      "0.026480671474169524\n",
      "0.49312089616532234\n",
      "\n",
      "0.4499633308887259\n",
      "0.0266635205285512\n",
      "0.49320682024624324\n",
      "\n",
      "0.44976331755450366\n",
      "0.02688362221436151\n",
      "0.49332699965997384\n",
      "\n",
      "0.44742982865524367\n",
      "0.027001643578304767\n",
      "0.49335578032172916\n",
      "\n",
      "0.44536302420161344\n",
      "0.02713132966904456\n",
      "0.49339578174031035\n",
      "\n",
      "0.4427628508567238\n",
      "0.027234636871508382\n",
      "0.49341252966585447\n",
      "\n",
      "0.44242949529968667\n",
      "0.027445051750203515\n",
      "0.49352720072369805\n",
      "\n",
      "0.44082938862590837\n",
      "0.027594202898550725\n",
      "0.4935870632394456\n",
      "\n",
      "0.4340289352623508\n",
      "0.027494558368656205\n",
      "0.4934196519842618\n",
      "\n",
      "0.432962197479832\n",
      "0.02766662855836287\n",
      "0.49350235498424205\n",
      "\n",
      "0.4327621841456097\n",
      "0.027879341864716634\n",
      "0.49362321416708677\n",
      "\n",
      "0.4325621708113874\n",
      "0.028091812264474134\n",
      "0.4937441092017752\n",
      "\n",
      "0.42722848189879326\n",
      "0.02805747256476977\n",
      "0.4936385123510884\n",
      "\n",
      "0.4254283618907927\n",
      "0.02819124943617501\n",
      "0.4936886134228739\n",
      "\n",
      "0.4246283085539036\n",
      "0.02837198829092547\n",
      "0.49378320890599275\n",
      "\n",
      "0.4224281618774585\n",
      "0.028484916451721434\n",
      "0.49381540909035865\n",
      "\n",
      "0.41936129075271683\n",
      "0.028555493586168428\n",
      "0.4938085176201272\n",
      "\n",
      "0.41469431295419695\n",
      "0.028549297333185793\n",
      "0.493728611970362\n",
      "\n",
      "0.4130942062804187\n",
      "0.0286880723822134\n",
      "0.4937870186744268\n",
      "\n",
      "0.4120941396093073\n",
      "0.028854625550660793\n",
      "0.49387274320264046\n",
      "\n",
      "0.4116274418294553\n",
      "0.029046099680932993\n",
      "0.4939828166851075\n",
      "\n",
      "0.41049403293552905\n",
      "0.029205094422485724\n",
      "0.4940626342830727\n",
      "\n",
      "0.4082938862590839\n",
      "0.02931204199934376\n",
      "0.49409383324453426\n",
      "\n",
      "0.4072271484765651\n",
      "0.029472764981988864\n",
      "0.4941767901630074\n",
      "\n",
      "0.4017601173411561\n",
      "0.029421308815575985\n",
      "0.4940572485048389\n",
      "\n",
      "0.3956930462030802\n",
      "0.02934247162133219\n",
      "0.49390690777209795\n",
      "\n",
      "0.39555970398026535\n",
      "0.02954399486191394\n",
      "0.4940336066962535\n",
      "\n",
      "0.3949596639775985\n",
      "0.02972308350261948\n",
      "0.494138499858571\n",
      "\n",
      "0.39415961064070937\n",
      "0.0298921746557062\n",
      "0.4942341277316591\n",
      "\n",
      "0.38995933062204147\n",
      "0.02989821882951654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49416973866838443\n",
      "\n",
      "0.38449229948663244\n",
      "0.029844472467423288\n",
      "0.49404326333049936\n",
      "\n",
      "0.3845589705980399\n",
      "0.03005148681307135\n",
      "0.49418078770797363\n",
      "\n",
      "0.38449229948663244\n",
      "0.030252100840336135\n",
      "0.4943119657943207\n",
      "\n",
      "0.38449229948663244\n",
      "0.030455786599453895\n",
      "0.4944463677802782\n",
      "\n",
      "0.3842922861524102\n",
      "0.030649732339666214\n",
      "0.4945713005483245\n",
      "\n",
      "0.379425295019668\n",
      "0.03061862112059988\n",
      "0.4944734064902382\n",
      "\n",
      "0.37742516167744516\n",
      "0.03072451733444052\n",
      "0.49451229463940066\n",
      "\n",
      "0.3752250150010001\n",
      "0.030820146861102488\n",
      "0.49454145383655373\n",
      "\n",
      "0.37395826388425896\n",
      "0.030959752321981424\n",
      "0.494615853002107\n",
      "\n",
      "0.3727581838789253\n",
      "0.03110195674562307\n",
      "0.4946936245079858\n",
      "\n",
      "0.3716914460964064\n",
      "0.03125\n",
      "0.4947780215548928\n",
      "\n",
      "0.3710247349823322\n",
      "0.03141683778234086\n",
      "0.49488201107686114\n",
      "\n",
      "0.35802386825788385\n",
      "0.030995270202274328\n",
      "0.4943757153798901\n",
      "\n",
      "0.3578238549236616\n",
      "0.031180848923757786\n",
      "0.49450405727199115\n",
      "\n",
      "0.3546903126875125\n",
      "0.03122810529476529\n",
      "0.49448427393653027\n",
      "\n",
      "0.35215681045403024\n",
      "0.03130296082145349\n",
      "0.494494489075805\n",
      "\n",
      "0.34055603706913795\n",
      "0.03095914568433428\n",
      "0.49403071899379075\n",
      "\n",
      "0.3374891659443963\n",
      "0.03100926377376889\n",
      "0.4940089343361543\n",
      "\n",
      "0.33442229481965463\n",
      "0.03105891487916141\n",
      "0.49398639175261033\n",
      "\n",
      "0.3331555437029135\n",
      "0.031189461449050753\n",
      "0.494060877010752\n",
      "\n",
      "0.3284218947929862\n",
      "0.031162835433298067\n",
      "0.49394629741446766\n",
      "\n",
      "0.31802120141342755\n",
      "0.030885836096636667\n",
      "0.4935080510226986\n",
      "\n",
      "0.3151543436229082\n",
      "0.03094339622641509\n",
      "0.4934900009710804\n",
      "\n",
      "0.31202080138675914\n",
      "0.030988825241806746\n",
      "0.49345565568124083\n",
      "\n",
      "0.31108740582705513\n",
      "0.031129864041256453\n",
      "0.49354936696632473\n",
      "\n",
      "0.3052203480232015\n",
      "0.031055323105532308\n",
      "0.4933519261792659\n",
      "\n",
      "0.30402026801786786\n",
      "0.031183294663573086\n",
      "0.4934302336623133\n",
      "\n",
      "0.29715314354290284\n",
      "0.031066176470588232\n",
      "0.4931644094862749\n",
      "\n",
      "0.2864857657177145\n",
      "0.03079152327476906\n",
      "0.49264389663085506\n",
      "\n",
      "0.2844189612640843\n",
      "0.0308803611738149\n",
      "0.4926651484391479\n",
      "\n",
      "0.28241882792186146\n",
      "0.030971459439992802\n",
      "0.49269062955378795\n",
      "\n",
      "0.2816187745849723\n",
      "0.031112310044060783\n",
      "0.4927950562769793\n",
      "\n",
      "0.2805520368024535\n",
      "0.031241583625100995\n",
      "0.492882339947625\n",
      "\n",
      "0.27868524568304553\n",
      "0.031336735607485004\n",
      "0.49291724670893544\n",
      "\n",
      "0.277351823454897\n",
      "0.031453846841211684\n",
      "0.4929875636753573\n",
      "\n",
      "0.27335155677045136\n",
      "0.03145827779258865\n",
      "0.49287978036204577\n",
      "\n",
      "0.27028468564570973\n",
      "0.03150163702327228\n",
      "0.4928325448061728\n",
      "\n",
      "0.2694846323088206\n",
      "0.031639416703490936\n",
      "0.4929395407321524\n",
      "\n",
      "0.26921794786319087\n",
      "0.031799311014928014\n",
      "0.49308327185054507\n",
      "\n",
      "0.264084272284819\n",
      "0.03175438596491228\n",
      "0.4928928299933686\n",
      "\n",
      "0.2618841256083739\n",
      "0.03183209444687364\n",
      "0.49290327813833795\n",
      "\n",
      "0.25741716114407626\n",
      "0.031815020862308765\n",
      "0.49275170640637944\n",
      "\n",
      "0.2564837655843723\n",
      "0.03194444444444445\n",
      "0.49285166783882595\n",
      "\n",
      "0.2565504366957797\n",
      "0.03211526777189481\n",
      "0.4930237791020759\n",
      "\n",
      "0.25241682778851926\n",
      "0.03211048769961156\n",
      "0.4928936935028198\n",
      "\n",
      "0.25221681445429694\n",
      "0.03226919758412425\n",
      "0.4930484486900296\n",
      "\n",
      "0.25221681445429694\n",
      "0.03243616287094548\n",
      "0.49321792234029666\n",
      "\n",
      "0.24681645443029535\n",
      "0.03237687366167024\n",
      "0.4929931227712685\n",
      "\n",
      "0.24334955663710914\n",
      "0.03239832892829738\n",
      "0.492906025827953\n",
      "\n",
      "0.24114940996066406\n",
      "0.032471948316899014\n",
      "0.49291316966907534\n",
      "\n",
      "0.23674911660777384\n",
      "0.032454361054766734\n",
      "0.4927492363707524\n",
      "\n",
      "0.23468231215414362\n",
      "0.03253265908133164\n",
      "0.49276464268079356\n",
      "\n",
      "0.2312154143609574\n",
      "0.03255306653242722\n",
      "0.49266792149768973\n",
      "\n",
      "0.2270818054536969\n",
      "0.03254610698489527\n",
      "0.4925131311080909\n",
      "\n",
      "0.22634842322821522\n",
      "0.03267755918639546\n",
      "0.49263566926886926\n",
      "\n",
      "0.22074804986999133\n",
      "0.032610494951167024\n",
      "0.4923515067293106\n",
      "\n",
      "0.22028135209013935\n",
      "0.032751633446365065\n",
      "0.4924984788051075\n",
      "\n",
      "0.21414760984065603\n",
      "0.03266311038161674\n",
      "0.492154813213441\n",
      "\n",
      "0.2133475565037669\n",
      "0.03278957291581277\n",
      "0.49227503531063876\n",
      "\n",
      "0.2079471964797653\n",
      "0.03273082559843674\n",
      "0.491980554524196\n",
      "\n",
      "0.20774718314554302\n",
      "0.03288027997070074\n",
      "0.4921575343186637\n",
      "\n",
      "0.20768051203413562\n",
      "0.03303498779495524\n",
      "0.4923470760917137\n",
      "\n",
      "0.20774718314554302\n",
      "0.03319502074688797\n",
      "0.49254893297632796\n",
      "\n",
      "0.2044802986865791\n",
      "0.03321989953005996\n",
      "0.4924482971531777\n",
      "\n",
      "0.20321354756983798\n",
      "0.03332524468171156\n",
      "0.4925303317889888\n",
      "\n",
      "0.20248016534435628\n",
      "0.03345184227537169\n",
      "0.4926624054043855\n",
      "\n",
      "0.20034668977931863\n",
      "0.03352135374697825\n",
      "0.49266547932746413\n",
      "\n",
      "0.19987999199946663\n",
      "0.03365810451727193\n",
      "0.49282421255420783\n",
      "\n",
      "0.19887992532835522\n",
      "0.033772917336764234\n",
      "0.49293416607359\n",
      "\n",
      "0.19781318754583638\n",
      "0.03388469568010278\n",
      "0.4930387935958365\n",
      "\n",
      "0.19654643642909528\n",
      "0.03398797595190381\n",
      "0.4931256289591708\n",
      "\n",
      "0.1948796586439096\n",
      "0.03407454807230843\n",
      "0.49317569981699433\n",
      "\n",
      "0.1946129741982799\n",
      "0.03421810041573393\n",
      "0.4933580801518455\n",
      "\n",
      "0.19467964530968732\n",
      "0.03437524982012951\n",
      "0.4935720050224422\n",
      "\n",
      "0.1944129608640576\n",
      "0.034518577706751895\n",
      "0.493755381314664\n",
      "\n",
      "0.19287952530168678\n",
      "0.034609250398724076\n",
      "0.4938226973162105\n",
      "\n",
      "0.18807920528035202\n",
      "0.03456476930394799\n",
      "0.493581694874859\n",
      "\n",
      "0.1846123074871658\n",
      "0.03457530786233028\n",
      "0.49346010633982423\n",
      "\n",
      "0.18107873858257217\n",
      "0.03458303859152716\n",
      "0.49332659368042553\n",
      "\n",
      "0.1784118941262751\n",
      "0.03462593027810419\n",
      "0.4932768183431366\n",
      "\n",
      "0.17407827188479233\n",
      "0.03460099750623441\n",
      "0.49304865074392124\n",
      "\n",
      "0.17274484965664377\n",
      "0.0346973704683367\n",
      "0.49313521008466327\n",
      "\n",
      "0.17261150743382891\n",
      "0.034842121636335356\n",
      "0.49335122720050495\n",
      "\n",
      "0.16921128075205014\n",
      "0.03485400046472001\n",
      "0.49321726585508624\n",
      "\n",
      "0.16921128075205014\n",
      "0.03500348486021838\n",
      "0.4934518856494541\n",
      "\n",
      "0.16721114740982732\n",
      "0.035071456160679804\n",
      "0.49347035727951677\n",
      "\n",
      "0.1658777251816788\n",
      "0.03516619110048585\n",
      "0.49356237036824385\n",
      "\n",
      "0.161544102940196\n",
      "0.035138867577106034\n",
      "0.4933183866452677\n",
      "\n",
      "0.15627708513900926\n",
      "0.03507434235608082\n",
      "0.49294821773362546\n",
      "\n",
      "0.15494366291086073\n",
      "0.03516784654030601\n",
      "0.493039982497402\n",
      "\n",
      "0.14987665844389625\n",
      "0.03511161558834658\n",
      "0.49266702483312197\n",
      "\n",
      "0.14960997399826656\n",
      "0.03524695560093791\n",
      "0.4928950595530595\n",
      "\n",
      "0.14894326288419227\n",
      "0.0353661301292224\n",
      "0.49307373043092223\n",
      "\n",
      "0.14614307620508035\n",
      "0.03539956315432703\n",
      "0.4929812861482771\n",
      "\n",
      "0.14574304953663578\n",
      "0.035528791870530665\n",
      "0.49319862999817604\n",
      "\n",
      "0.1436095739715981\n",
      "0.03558825737667993\n",
      "0.4931929906639945\n",
      "\n",
      "0.1298753250216681\n",
      "0.03518888149626673\n",
      "0.4915023924147218\n",
      "\n",
      "0.12954196946463098\n",
      "0.035318457218856215\n",
      "0.4917493185421189\n",
      "\n",
      "0.12947529835322355\n",
      "0.0354583733471227\n",
      "0.492039391527738\n",
      "\n",
      "0.1023401560104007\n",
      "0.034561881543094794\n",
      "0.48702702664642733\n",
      "\n",
      "0.10194012934195613\n",
      "0.03468539486885481\n",
      "0.4873005336889445\n",
      "\n",
      "0.10147343156210414\n",
      "0.03480627372341187\n",
      "0.48756029440058624\n",
      "\n",
      "0.10020668044536303\n",
      "0.03489702517162471\n",
      "0.48762274611185885\n",
      "\n",
      "0.10007333822254817\n",
      "0.035030025736345435\n",
      "0.48797309084084184\n",
      "\n",
      "0.09333955597039803\n",
      "0.03491590376836279\n",
      "0.48656786674868585\n",
      "\n",
      "0.09280618707913861\n",
      "0.03503297638465357\n",
      "0.4868284265772036\n",
      "\n",
      "0.0893392892859524\n",
      "0.035040621688449305\n",
      "0.4862225648764804\n",
      "\n",
      "0.0893392892859524\n",
      "0.035176944267853366\n",
      "0.4866553817723111\n",
      "\n",
      "0.08753916927795187\n",
      "0.0352460172000564\n",
      "0.48654005747481194\n",
      "\n",
      "0.08700580038669245\n",
      "0.03536207382361228\n",
      "0.4868192272501176\n",
      "\n",
      "0.08693912927528502\n",
      "0.035495457426579335\n",
      "0.48724721857033526\n",
      "\n",
      "0.08660577371824789\n",
      "0.03561875263972969\n",
      "0.48759633529478874\n",
      "\n",
      "0.08253883592239483\n",
      "0.035601653935104074\n",
      "0.4867614627952185\n",
      "\n",
      "0.08013867591172745\n",
      "0.03564688613965192\n",
      "0.4864211399484589\n",
      "\n",
      "0.07887192479498634\n",
      "0.03573422668900056\n",
      "0.48646755334409886\n",
      "\n",
      "0.07720514700980065\n",
      "0.03580633925461511\n",
      "0.4863672301310228\n",
      "\n",
      "0.07647176478431895\n",
      "0.035913140311804\n",
      "0.48661335596597405\n",
      "\n",
      "0.07433828921928129\n",
      "0.03596722677405916\n",
      "0.4863291369599448\n",
      "\n",
      "0.07293819587972532\n",
      "0.03604852686308491\n",
      "0.4863202446285008\n",
      "\n",
      "0.07227148476565104\n",
      "0.03615709634965713\n",
      "0.4866092859615231\n",
      "\n",
      "0.07120474698313221\n",
      "0.03625043237634037\n",
      "0.48674283198374135\n",
      "\n",
      "0.07027135142342823\n",
      "0.036348559187340194\n",
      "0.4869365193323645\n",
      "\n",
      "0.06753783585572372\n",
      "0.03637866887143448\n",
      "0.4863566825207766\n",
      "\n",
      "0.06713780918727916\n",
      "0.0364963503649635\n",
      "0.4867940292985997\n",
      "\n",
      "0.06467097806520435\n",
      "0.03653595220108509\n",
      "0.48629090385988355\n",
      "\n",
      "0.06347089805987066\n",
      "0.03662300253754886\n",
      "0.4863675834752488\n",
      "\n",
      "0.06347089805987066\n",
      "0.036755125831447584\n",
      "0.4870354138235986\n",
      "\n",
      "0.059803986932462166\n",
      "0.03674863387978142\n",
      "0.4858588728537173\n",
      "\n",
      "0.05987065804386959\n",
      "0.03688272658971382\n",
      "0.4866172169025022\n",
      "\n",
      "0.058603906927128475\n",
      "0.036966307461465016\n",
      "0.4866716677994455\n",
      "\n",
      "0.058670578038535905\n",
      "0.037100184136943326\n",
      "0.48745619531667017\n",
      "\n",
      "0.057870524701646775\n",
      "0.03720106288751107\n",
      "0.4877901511591733\n",
      "\n",
      "0.05607040469364624\n",
      "0.03726370189038487\n",
      "0.48758749808902463\n",
      "\n",
      "0.052136809120608044\n",
      "0.03724520891176271\n",
      "0.48600814869828196\n",
      "\n",
      "0.052003466897793185\n",
      "0.037370523322726966\n",
      "0.4868090412800523\n",
      "\n",
      "0.05140342689512634\n",
      "0.03747801380056826\n",
      "0.48731988245529634\n",
      "\n",
      "0.04980332022134809\n",
      "0.0375472717450027\n",
      "0.4871776622154389\n",
      "\n",
      "0.04813654243616241\n",
      "0.037613751263902935\n",
      "0.48696821287498665\n",
      "\n",
      "0.04740316021068071\n",
      "0.03771551724137931\n",
      "0.4874503669174968\n",
      "\n",
      "0.047336489099273286\n",
      "0.03784256952393778\n",
      "0.4884503366238943\n",
      "\n",
      "0.04580305353690246\n",
      "0.037913417585372415\n",
      "0.488396840040171\n",
      "\n",
      "0.045869724648309886\n",
      "0.03804530483296363\n",
      "0.4895648926911088\n",
      "\n",
      "0.04553636909127275\n",
      "0.038161784466541254\n",
      "0.49045958931722505\n",
      "\n",
      "0.0439362624174945\n",
      "0.03822937625754527\n",
      "0.49047338777363114\n",
      "\n",
      "0.04353623574904994\n",
      "0.03834294141305805\n",
      "0.491409756568772\n",
      "\n",
      "0.040269351290086006\n",
      "0.03834591489077427\n",
      "0.490085564899799\n",
      "\n",
      "0.03993599573304887\n",
      "0.038461538461538464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4912360864645381\n",
      "\n",
      "0.03686912460830722\n",
      "0.038471778487752926\n",
      "0.489813909263787\n",
      "\n",
      "0.03613574238282552\n",
      "0.03857152357518122\n",
      "0.490755210828848\n",
      "\n",
      "0.033935595706380425\n",
      "0.038614649681528654\n",
      "0.4900259271320548\n",
      "\n",
      "0.032535502366824454\n",
      "0.03868830738655184\n",
      "0.4902603555785275\n",
      "\n",
      "0.028468564570971397\n",
      "0.03865945375379338\n",
      "0.4850373130409962\n",
      "\n",
      "0.027601840122674845\n",
      "0.038753048177684046\n",
      "0.4860741580300513\n",
      "\n",
      "0.025001666777785186\n",
      "0.03878007098724859\n",
      "0.4804782715017078\n",
      "\n",
      "0.02326821788119208\n",
      "0.038840047237895296\n",
      "0.4748172465697774\n",
      "\n",
      "0.021001400093339555\n",
      "0.03887943448095301\n",
      "0.4384859698987086\n",
      "\n",
      "0.021068071204746984\n",
      "0.039007788467831664\n",
      "0.45994725949662857\n",
      "\n",
      "0.020334688979265284\n",
      "0.039105414595867125\n",
      "0.4385434136301266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for eps in eps_list:\n",
    "    answers = np.where(sr < eps, 1.0, 0.0)\n",
    "    print( accuracy_score(answers, tr) )\n",
    "    print( f1_score(answers, tr) )\n",
    "    print( roc_auc_score(answers, tr) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что метрики ведут себя одинаково плохо, даже если в качестве порогов брать amonaly score самих аномальных объектов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecea9bc5e774fea8d222e6061dfef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 50.7 s, sys: 6.74 s, total: 57.4 s\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "alg = OneClassSVM(gamma=0.001, nu=0.03, verbose=True)\n",
    "\n",
    "# time = np.array( [50, 100, 200, 300, 500, 1000, 5000] )\n",
    "\n",
    "batch_size = 100\n",
    "svd = TruncatedSVD(n_components=10, n_iter=3, random_state=42)\n",
    "vectors = None\n",
    "\n",
    "for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "    batch = pp.preprocess1(batch)\n",
    "    v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "    vectors = v1.vectorize(batch)\n",
    "    vectors = svd.fit_transform(vectors)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]CPU times: user 2.4 ms, sys: 6.64 ms, total: 9.03 ms\n",
      "Wall time: 21.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = alg.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 577 µs, sys: 271 µs, total: 848 µs\n",
      "Wall time: 777 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = alg.predict(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем обертку над SVM.\n",
    "\n",
    "Будем содержать хранилище размера, не больше заданного размера, и обучать SVM по нему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 5000, nu=0.05):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.alg = OneClassSVM(nu=nu)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count].copy()\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        for target in self.alg.predict(X[start_count:]):\n",
    "            yield target\n",
    "            \n",
    "            \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ].copy()\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos].copy()\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f013b0811e47bd86420fcd1c315680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.36189079271951463\n",
      "0.04585784069384907\n",
      "0.560148082636007\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ea517fe88b4b2b8ab391fc4313284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7886192412827522\n",
      "0.03822235704535113\n",
      "0.5052143002925369\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7e5351ef4c4fec9728a531ad6f4324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4558303886925795\n",
      "0.03878464316558059\n",
      "0.501409997656681\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12951e6a0d794cb6b9bd26e8eed840a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.643042869524635\n",
      "0.037136948116176606\n",
      "0.49665473274825955\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aab0fee7584436086922986b15c582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.47273818254550304\n",
      "0.03963666391412056\n",
      "0.5076418804000272\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d5e838c6cc43f983430780978006cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.594461853012423\n",
      "0.03770500448241312\n",
      "0.49785482383533264\n",
      "\n",
      "CPU times: user 1min 31s, sys: 12.9 s, total: 1min 44s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 500\n",
    "vectorization = 'tf-idf'\n",
    "svd = TruncatedSVD(n_components=10, n_iter=5, random_state=42)\n",
    "v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pp = Preprocessor()\n",
    "\n",
    "for nu in [0.00001, 0.0001, 0.0005, 0.0015, 0.005, 0.015]:\n",
    "    alg = SVM(nu=nu)\n",
    "    \n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        vectors = v1.vectorize(batch)\n",
    "        vectors = svd.fit_transform(vectors)\n",
    "        y_pred += list( alg.check_novelty(vectors) )\n",
    "        y_true += target\n",
    "\n",
    "    y_pred = np.where(np.array(y_pred) == 1, 0, 1).tolist()\n",
    "\n",
    "    print( accuracy_score(y_true, y_pred) )\n",
    "    print( f1_score(y_true, y_pred) )\n",
    "    try:\n",
    "        print( roc_auc_score(y_true, y_pred) )\n",
    "    except:\n",
    "        print('roc auc failed')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На будущее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "идея параллельного обучения и предсказания:\n",
    "- один процесс работает с потоком данных, берет текущий обученный svm, предсказывает на нем результат\n",
    "также он проверяет, что в бакете не накопилось bucket_max_size объектов.\n",
    "- как только накопилось - запускаем второй процесс, который обучает svm на объектах старых + из бакета, мб \n",
    "рандомно выкидываем bucket_max_size объектов из нашего хранилища\n",
    "\n",
    "По факту даже параллельно не надо делать, svm на 5к примерах быстрее обучается, чем мы получаем векторы. Тогда надо просто иметь хранилище данных фиксированного размера, в котором обновлять данные и обучать svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- сделать обертку над OneClassSVM, в котором будет хранилище, дообучение на каждом батче и предикт\n",
    "- запустить его на текущих данных\n",
    "- обкачать сайт и сделать новый датасет\n",
    "- проверить LOF из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor.fit\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "\n",
    "https://towardsdatascience.com/anomaly-detection-with-local-outlier-factor-lof-d91e41df10f2\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/support-vector-machine-svm-for-anomaly-detection-73a8d676c331\n",
    "\n",
    "обкачать\n",
    "http://famouspoetsandpoems.com/poets/william_blake/poems/1192\n",
    "\n",
    "BERT для извлечения признаков:\n",
    "https://habr.com/ru/post/487358/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно собрать также датасеты с разной тематикой и обучить нейронку находить различия тем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fizzy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
