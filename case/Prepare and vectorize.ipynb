{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель: написать генератор, принимающий батч твиттов из твиттер-датасета, и выдающий по этому батчу предобработанные, векторизованные данные с отобранными признаками. Необходимо использовать различные методы решения каждой из задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Dmitry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Dmitry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/Dmitry/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import twitter_samples\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения исследования, скачаем весь датасет, и будем собирать батчи из него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_dataset.csv', low_memory=False)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399424, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generator(batch_size=10):\n",
    "    \n",
    "    # df = pd.read_csv('twitter_dataset.csv')\n",
    "    for i in range(0, df.shape[0], batch_size):\n",
    "        stream_batch = df.iloc[i : min(i + batch_size, df.shape[0])]\n",
    "        yield stream_batch['content'].tolist(), stream_batch['novel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Tune in 11:30 ET tomorrow for a live webcast of Families USA Presidential Forum on health care: http://presidentialforums.health08.org/', \"Iowans, there's a good chance there's a Biden near you today on a cool 14 F day: http://blog.joebiden.com/?p=1625\", 'Met with Judge Sotomayor today. Very impressive, experienced judge who is also very down to earth. Will do great work on Supreme Court.', 'PHOTOS: On June 12, Michael Bennet met with members of the Mile High Youth Corps.  http://tinyurl.com/kl8bvn', 'PHOTOS: Women for Bennet held a fundraiser for Michael with State Treasurer Cary Kennedy on Sunday.  http://tinyurl.com/la22ye', 'VIDEO: State Treasurer Cary Kennedy praises Michael Bennet at Women for Bennet event on June 14.  http://tinyurl.com/lts3m3', 'VIDEO: Michael Bennet Speaks at Women for Bennet event on June 14.  http://tinyurl.com/lejavb', \"It's great to be back in Colorado with my family for the weekend.\", 'PHOTOS: Tour with Sen. Mark Udall of Ascent Solar Technologies in Thornton. http://tinyurl.com/ko4egc', 'PHOTOS: Jefferson County Young Democrats Summer BBQ. http://tinyurl.com/knwa7u'], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "for batch in stream_generator():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем удалять по возможности все теги(@) и ссылки. Некоторые стоп-слова также удалим. Применим в одном варианте лемматизацию, в другом - стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.methods = {\n",
    "            'lemm' : self.lemmatization,\n",
    "            'stem' : self.stemming\n",
    "        }\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.swords = set( stopwords.words(\"english\") )\n",
    "     \n",
    "    def preprocess1(self, content_batch, standard='lemm'):\n",
    "        # stream_batch = ([content], [novel]), content = list(string), novel = list\n",
    "    \n",
    "        preprocessed_batch = []\n",
    "        for doc in content_batch:\n",
    "            doc = doc.lower()\n",
    "            doc = self.delete_tags(doc)\n",
    "            doc = self.delete_links(doc)\n",
    "            doc = self.delete_garbage(doc)\n",
    "            tokens = self.get_tokens(doc)\n",
    "            tokens = self.methods[standard](tokens)\n",
    "            tokens = self.delete_stop_words(tokens)\n",
    "            preprocessed_batch.append( ' '.join(tokens) )\n",
    "            \n",
    "        return preprocessed_batch\n",
    "    \n",
    "    \n",
    "    def delete_tags(self, doc):\n",
    "        doc = re.sub(r'^@[\\w]*', ' ', doc) \n",
    "        doc = re.sub(r'\\s@[\\w]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_links(self, doc):\n",
    "        doc = re.sub(r'http\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'https\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'ftp\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'www\\.[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_garbage(self, doc):\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "        doc = re.sub(r\"[^a-zA-Z0-9\\s\\']*\", '', doc) # TODO: посмотреть что оставляет CountVectorizer\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_stop_words(self, tokens): # TODO: create own sw list\n",
    "        # new_tokens = []\n",
    "        return list( filter(lambda sword: sword not in self.swords, tokens) )\n",
    "#         for sword in swords:\n",
    "#             if sword not in stop_words:\n",
    "#                 new_tokens.append(sword)\n",
    "#         return new_tokens\n",
    "    \n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        return list(map(lambda token: token.lower(), doc.split()))\n",
    "    \n",
    "    \n",
    "    def lemmatization(self, tokens):\n",
    "        return list(map(lambda token: self.lemmatizer.lemmatize(token), tokens))\n",
    "    \n",
    "    \n",
    "    def stemming(self, tokens):\n",
    "        return list( map(lambda token: self.stemmer.stem(token), tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "doc = \"\"\"Hello there, @dmkalash, @lom and others! I've written my pp, so i want to | test > it. \n",
    "    see my https://www.site.com website.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there, ,  and others! I've written my pp, so i want to | test > it. \\n    see my https://www.site.com website.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.delete_tags(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there, @dmkalash, @lom and others! I've written my pp, so i want to | test > it. \\n    see my   website.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.delete_links(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there dmkalash lom and others I've written my pp so i want to  test  it see my httpswwwsitecom website\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.delete_garbage(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'there,',\n",
       " '@dmkalash,',\n",
       " '@lom',\n",
       " 'and',\n",
       " 'others!',\n",
       " \"i've\",\n",
       " 'written',\n",
       " 'my',\n",
       " 'pp,',\n",
       " 'so',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " '|',\n",
       " 'test',\n",
       " '>',\n",
       " 'it.',\n",
       " 'see',\n",
       " 'my',\n",
       " 'https://www.site.com',\n",
       " 'website.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pp.get_tokens(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'there,',\n",
       " '@dmkalash,',\n",
       " '@lom',\n",
       " 'and',\n",
       " 'others!',\n",
       " \"i've\",\n",
       " 'written',\n",
       " 'my',\n",
       " 'pp,',\n",
       " 'so',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " '|',\n",
       " 'test',\n",
       " '>',\n",
       " 'it.',\n",
       " 'see',\n",
       " 'my',\n",
       " 'https://www.site.com',\n",
       " 'website.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_tokens = pp.lemmatization(tokens)\n",
    "lemm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'there,',\n",
       " '@dmkalash,',\n",
       " '@lom',\n",
       " 'and',\n",
       " 'others!',\n",
       " \"i'v\",\n",
       " 'written',\n",
       " 'my',\n",
       " 'pp,',\n",
       " 'so',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " '|',\n",
       " 'test',\n",
       " '>',\n",
       " 'it.',\n",
       " 'see',\n",
       " 'my',\n",
       " 'https://www.site.com',\n",
       " 'website.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.stemming(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'there,',\n",
       " '@dmkalash,',\n",
       " '@lom',\n",
       " 'others!',\n",
       " \"i've\",\n",
       " 'written',\n",
       " 'pp,',\n",
       " 'want',\n",
       " '|',\n",
       " 'test',\n",
       " '>',\n",
       " 'it.',\n",
       " 'see',\n",
       " 'https://www.site.com',\n",
       " 'website.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.delete_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем всё вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello others i've written pp want test see website\",\n",
       " \"hello others i've written pp want test see website\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"\"\"Hello there, @dmkalash, @lom and others! I've written my pp, so i want to | test > it. \n",
    "    see my https://www.site.com website.\"\"\",\n",
    "          \"\"\"Hello there, @dmkalash, @lom and others! I've written my pp, so i want to | test > it. \n",
    "    see my https://www.site.com website.\"\"\"]\n",
    "pp.preprocess1(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# doc = nlp(sentence)\n",
    "\n",
    "# \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем твиттер-корпус, и создадим из него фиксированный словарь для векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples.fileids()\n",
    "vocab_corpus = ([(t, \"pos\") for t in twitter_samples.strings(\"positive_tweets.json\")] + \n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"negative_tweets.json\")] +\n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"tweets.20150430-223406.json\")]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "('#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', 'pos')\n"
     ]
    }
   ],
   "source": [
    "print( len(vocab_corpus) )\n",
    "print( vocab_corpus[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "vocab_corpus = list( map( lambda pair: pair[0], vocab_corpus ) )\n",
    "print( len(vocab_corpus) )\n",
    "print( vocab_corpus[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает DocToVec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hello', 'darkness', 'my', 'old', 'friend'], tags=[1]), TaggedDocument(words=['hello', 'darkness', 'my', 'old', 'friend'], tags=[2])]\n"
     ]
    }
   ],
   "source": [
    "tokens_train = ['hello', 'darkness', 'my', 'old', 'friend']\n",
    "tokens_test = ['hello', 'world', 'and', 'others', 'friend']\n",
    "\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(tokens_train, [1]),\n",
    "                gensim.models.doc2vec.TaggedDocument(tokens_train, [2])]\n",
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=3, min_count=1, epochs=1)\n",
    "\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03465455  0.13094941 -0.03809212]\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, в котором будут реализованы основные методы векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self, method, train_corpus):\n",
    "        self.methods = {\n",
    "            'one-hot' : self.one_hot_vectorizer,\n",
    "            'count' : self.count_vectorizer,\n",
    "            'tf-idf' : self.tfidf_vectorizer,\n",
    "            #'n-gramms' : self.n_gramms_vectorizer,\n",
    "            'doc-to-vec' : self.doc_to_vec_vectorizer\n",
    "        }\n",
    "        if method not in self.methods:\n",
    "            raise Exception('Wrong method: {}'.format(method))\n",
    "        \n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.train_corpus = Preprocessor().preprocess1( train_corpus )\n",
    "        \n",
    "    \n",
    "    def vectorize(self, batch, **args):\n",
    "        return self.methods[self.method](batch, **args)\n",
    "        \n",
    "    \n",
    "    def one_hot_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(binary=True, **args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def count_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def tfidf_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = TfidfVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "        \n",
    "        \n",
    "    def doc_to_vec_vectorizer(self, batch, **args):\n",
    "        \n",
    "        def extract_tokens(train = False):\n",
    "            for i, doc in enumerate(batch):\n",
    "                tokens = Preprocessor().get_tokens(doc)\n",
    "                if train:\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])    \n",
    "                else:\n",
    "                    yield tokens\n",
    "                    \n",
    "        \n",
    "        if self.model is None:\n",
    "            vocab = list( extract_tokens(train=True) )\n",
    "            self.model = gensim.models.doc2vec.Doc2Vec(min_count=1)\n",
    "            # self.model.build_vocab(train_corpus, update = True)\n",
    "            self.model.build_vocab(train_corpus)\n",
    "            self.model.train(vocab, total_examples=model.corpus_count, epochs=1, **args)\n",
    "        \n",
    "        return np.array( list( map( lambda token: self.model.infer_vector(token), extract_tokens() ) ) )\n",
    "    \n",
    "\"\"\"\n",
    "One-Hot\n",
    "CountVectorizer \n",
    "TF-IDF\n",
    "DocToVec\n",
    "n-грамм для 2 и 3\n",
    "...\n",
    "\n",
    "[0,1,0] -> svd\n",
    "[0,0,0,0,1,0,1] -> svd\n",
    "\n",
    "\"\"\"\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем работоспособность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['hello darkness my old friend', 'i have come to told with you again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x19748 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer('one-hot', vocab_corpus).vectorize(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x19748 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer('count', vocab_corpus).vectorize(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x19748 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer('tf-idf', vocab_corpus).vectorize(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer('doc-to-vec', vocab_corpus).vectorize(docs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собираем вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [05:56,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "shapes = []\n",
    "for batch, target in tqdm( stream_generator(10000) ):\n",
    "    batch = pp.preprocess1(batch)\n",
    "    v1 = Vectorizer('tf-idf', vocab_corpus)\n",
    "    vectors = v1.vectorize(batch)\n",
    "    shapes.append(vectors.shape)\n",
    "\n",
    "print(len(shapes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
