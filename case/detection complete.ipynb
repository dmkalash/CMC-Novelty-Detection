{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('twitter_samples')\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from nltk.corpus import gutenberg?\n",
    "from nltk.corpus import twitter_samples\n",
    "import gensim\n",
    "\n",
    "from multiprocessing.dummy import Pool, Queue\n",
    "\n",
    "import itertools\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier, KDTree\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения исследования, скачаем весь датасет, и будем собирать батчи из него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_dataset_4_9000_2000.csv', low_memory=False)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def stream_generator(batch_size=10):\n",
    "    for i in range(0, df.shape[0], batch_size):\n",
    "        stream_batch = df.iloc[i : min(i + batch_size, df.shape[0])]\n",
    "        yield stream_batch['content'].tolist(), stream_batch['novel'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые стоп-слова удалим. Применим в одном варианте лемматизацию, в другом - стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.methods = {\n",
    "            'lemm' : self.lemmatization,\n",
    "            'stem' : self.stemming,\n",
    "            'both' : self.both_norm\n",
    "        }\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.swords = set( stopwords.words(\"english\") )\n",
    "     \n",
    "    def preprocess1(self, content_batch, standard='both'):\n",
    "        # stream_batch: ([content], [novel]), content = list(string), novel = list\n",
    "    \n",
    "        preprocessed_batch = []\n",
    "        for doc in content_batch:\n",
    "            doc = doc.lower()\n",
    "            #doc = self.delete_tags(doc)\n",
    "            doc = self.delete_links(doc)\n",
    "            doc = self.delete_garbage(doc)\n",
    "            tokens = self.get_tokens(doc)\n",
    "            tokens = self.methods[standard](tokens)\n",
    "            tokens = self.delete_stop_words(tokens)\n",
    "            preprocessed_batch.append( ' '.join(tokens) )\n",
    "            \n",
    "        return preprocessed_batch\n",
    "    \n",
    "    \n",
    "    def delete_tags(self, doc):\n",
    "        # doc = re.sub(r'^@[\\w]*', ' ', doc) \n",
    "        # doc = re.sub(r'\\s@[\\w]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_links(self, doc):\n",
    "        doc = re.sub(r'http\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'https\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'ftp\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'www\\.[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_garbage(self, doc):\n",
    "        doc = re.sub(r'&amp;', ' ', doc)\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "        # doc = re.sub(r\"\\b\\w*\\W+\\w*\\b\", ' ', doc)\n",
    "        doc = re.sub(r\"[^a-zA-Z0-9\\s]*\", '', doc)\n",
    "        # doc = re.sub(r\"\\sRT\\s \", '', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_stop_words(self, tokens): # TODO: create own sw list\n",
    "        return list( filter(lambda sword: sword not in self.swords, tokens) )\n",
    "    \n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        return list(map(lambda token: token.lower(), doc.split()))\n",
    "    \n",
    "    \n",
    "    def lemmatization(self, tokens):\n",
    "        return list(map(lambda token: self.lemmatizer.lemmatize(token), tokens))\n",
    "    \n",
    "    \n",
    "    def stemming(self, tokens):\n",
    "        return list( map(lambda token: self.stemmer.stem(token), tokens) )\n",
    "    \n",
    "    \n",
    "    def both_norm(self, tokens):\n",
    "        tokens = list(map(lambda token: self.lemmatizer.lemmatize(token), tokens))\n",
    "        return list( map(lambda token: self.stemmer.stem(token), tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем корпус из классических произведений, и создадим из него фиксированный словарь для векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "followfriday franceint pkuchly57 milipolpari top engag member commun thi week\n"
     ]
    }
   ],
   "source": [
    "def create_vocab():\n",
    "    twitter_samples.fileids()\n",
    "    vocab_corpus = ([(t, \"pos\") for t in twitter_samples.strings(\"positive_tweets.json\")] + \n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"negative_tweets.json\")] +\n",
    "             [(t, \"neg\") for t in twitter_samples.strings(\"tweets.20150430-223406.json\")]\n",
    "            )\n",
    "    vocab_corpus = list( map( lambda pair: pair[0], vocab_corpus ) )\n",
    "    return vocab_corpus\n",
    "\n",
    "vocab_corpus = create_vocab()\n",
    "\n",
    "pp = Preprocessor()\n",
    "vocab_corpus = pp.preprocess1(vocab_corpus)\n",
    "\n",
    "print( len(vocab_corpus) )\n",
    "print( vocab_corpus[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, в котором будут реализованы основные методы векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self, method, train_corpus):\n",
    "        self.methods = {\n",
    "            'one-hot' : self.one_hot_vectorizer,\n",
    "            'count' : self.count_vectorizer,\n",
    "            'tf-idf' : self.tfidf_vectorizer,\n",
    "            #'n-gramms' : self.n_gramms_vectorizer,\n",
    "            'doc-to-vec' : self.doc_to_vec_vectorizer\n",
    "        }\n",
    "        if method not in self.methods:\n",
    "            raise Exception('Wrong method: {}'.format(method))\n",
    "        \n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.train_corpus = Preprocessor().preprocess1( train_corpus )\n",
    "        \n",
    "    \n",
    "    def vectorize(self, batch, **args):\n",
    "        return self.methods[self.method](batch, **args)\n",
    "        \n",
    "    \n",
    "    def one_hot_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(binary=True, **args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def count_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def tfidf_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = TfidfVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "        \n",
    "        \n",
    "    def doc_to_vec_vectorizer(self, batch, **args):\n",
    "        \n",
    "        def extract_tokens(train = False):\n",
    "            if train:\n",
    "                for i, doc in enumerate(self.train_corpus):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])    \n",
    "                    \n",
    "            else:\n",
    "                for i, doc in enumerate(batch):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield tokens\n",
    "\n",
    "        \n",
    "        if self.model is None:\n",
    "            # vocab = list( extract_tokens(train=True) )\n",
    "            vocab = list( extract_tokens(train=True) )\n",
    "            self.model = gensim.models.doc2vec.Doc2Vec(min_count=1, vector_size=100)\n",
    "            # self.model.build_vocab(train_corpus, update = True)\n",
    "            # self.model.build_vocab(self.train_corpus)\n",
    "            self.model.build_vocab( vocab )\n",
    "            self.model.train(vocab, total_examples=self.model.corpus_count, epochs=5, **args)\n",
    "        \n",
    "        return np.array( list( map( lambda token: self.model.infer_vector(token), extract_tokens() ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты замеров есть в таблице:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1a6M76eS8L-WTGKVI-xRajut2P-X70j-bv5s_7WZv3ZE/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scores(pipeline, batch_size, n_start, return_target=False):\n",
    "    \"\"\"\n",
    "        pipeline: pp, alg, svd, vectorizer\n",
    "        return: accuracy, precision, recall, roc_auc, [answers, targets, scores]\n",
    "    \"\"\"\n",
    "    \n",
    "    pp, alg, svd, vectorizer = pipeline\n",
    "    \n",
    "    answers = []\n",
    "    targets = []\n",
    "\n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        vectors = vectorizer.vectorize(batch)\n",
    "        if svd is not None:\n",
    "            vectors = svd.fit_transform(vectors)\n",
    "            \n",
    "        answers += list( alg.predict(vectors) ) \n",
    "        targets += target\n",
    "    \n",
    "    answers = answers[n_start:]\n",
    "    targets = targets[n_start:]\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(targets, answers)\n",
    "    except:\n",
    "        roc_auc = 0.5\n",
    "        \n",
    "    result = [roc_auc]\n",
    "    \n",
    "    if return_target:\n",
    "        result += [answers, targets]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def combine_params(*args):\n",
    "    return list( itertools.product(*args) )\n",
    "\n",
    "\n",
    "def pool_process(params, run_with_param):\n",
    "    \n",
    "    def process_run(arg):\n",
    "        i, run_with_param = arg\n",
    "\n",
    "        #with open('data/poems{:05d}.txt'.format(i), mode='w') as txt:\n",
    "        while not queue.empty():\n",
    "            ind = queue.get()\n",
    "            cur_param = params[ind]\n",
    "        \n",
    "            try:\n",
    "                roc_auc = run_with_param(cur_param)\n",
    "            except Exception as err:\n",
    "                roc_auc = -1\n",
    "                with lock:\n",
    "                    print('process: {}; error: {}; \\nparams: {}'.format( i, err, cur_param))\n",
    "\n",
    "            with lock:\n",
    "                print(cur_param)\n",
    "                print('ROC AUC: {}'.format(roc_auc))\n",
    "                print()\n",
    "                pbar.update(1)\n",
    "    \n",
    "    \n",
    "    queue = Queue()\n",
    "\n",
    "    params_ind = np.arange(len(params))\n",
    "    for i in params_ind:\n",
    "        queue.put(i)\n",
    "\n",
    "    with Pool(processes=4) as pool, tqdm(total=queue.qsize()) as pbar:\n",
    "        lock = pbar.get_lock()\n",
    "        pool.map(process_run, zip(range(pool._processes), [run_with_param] * len(range(pool._processes)) ))\n",
    "\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый класс для алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    \n",
    "    def __init__(self, n_start, max_samples):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "            \n",
    "    def update_storage(self, X, start_count): # Warning: copying removed \n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ] # .copy()\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos] # .copy()\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :] # .copy()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :] # .copy()\n",
    "    \n",
    "    \n",
    "    def check_consistency(self, X):\n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        \n",
    "        \n",
    "class NovelPredictor(BaseEstimator):\n",
    "    \"\"\"\n",
    "        all algorithms must contain fit and predict methods\n",
    "        all sons must implement 'decision_function' method\n",
    "    \"\"\"\n",
    "    def __init__(self, alg, n_start=100, max_samples = 1000 ):\n",
    "        self.storage = Storage(n_start, max_samples)\n",
    "        self.alg = alg\n",
    "        self.novelty_label = 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        self.storage.check_consistency(X)\n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.storage.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.storage.n_for_start, X.shape[0])\n",
    "            self.storage.data = X[:start_count]\n",
    "            self.storage.pos = start_count % self.storage.max_samples\n",
    "            self.storage.initialized = True\n",
    "        \n",
    "        targets = []\n",
    "        if start_count < X.shape[0]:\n",
    " \n",
    "            # self.storage.update_storage(X, start_count) # TODO: так после же надо?\n",
    "            self.alg.fit(self.storage.data)\n",
    "            # print(len( self.decision_function( self.alg.predict(X[start_count:]) ) ), start_count)\n",
    "            predict = self.decision_function( self.alg.predict(X[start_count:]) )\n",
    "            targets = [self.novelty_label] * start_count + predict # SHOULD BE LIST!\n",
    "            self.storage.update_storage(X, start_count) # TODO: так после же надо?\n",
    "            \n",
    "        else:\n",
    "            targets = [self.novelty_label] * start_count\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.storage.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQR:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X.copy()\n",
    "    def predict(self, X, y=None):\n",
    "        q75, q25 = np.percentile(self.X, [75, 25], axis=0)\n",
    "        iqr = q75 - q25\n",
    "        scores = np.where( (X > q25 - 1.5 * iqr).astype(int) * (X < q75 + 1.5 * iqr).astype(int) == 1, 0, 1)\n",
    "        scores = np.sum( scores, axis=1 )\n",
    "        #return np.where(scores > 0, 1, 0)\n",
    "        return softmax( scores ) # неверно. Нужно сначала собрать все скоры, потом сделать predict\n",
    "\n",
    "\n",
    "class IQRModel(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500):\n",
    "        \n",
    "        alg = IQR()\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        if not isinstance(predict, list):\n",
    "            return predict.tolist()\n",
    "        \n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbscanWrapper:\n",
    "    def __init__(self, eps, min_samples):\n",
    "        self.alg = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        self.alg.fit( np.concatenate( (self.X, X), axis=0) )\n",
    "        return np.where( self.alg.labels_[self.X.shape[0]:] < 0, 1, 0)\n",
    "\n",
    "\n",
    "class Dbscan(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, eps=0.4, min_samples=10):\n",
    "        \n",
    "        alg = DbscanWrapper(eps=eps, min_samples=min_samples)\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elliptical Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EllipseWrapper:\n",
    "    \n",
    "    def __init__(self, contamination):\n",
    "        self.alg = EllipticEnvelope(contamination=contamination)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.alg.fit(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return -alg.decision_function(X)\n",
    "        #return 1 - softmax( alg.decision_function(X) )\n",
    "\n",
    "class Ellipse(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, contamination, n_start=100, max_samples = 500):\n",
    "        \n",
    "        alg = EllipticEnvelope(contamination=contamination)\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist()\n",
    "        #return np.where( predict < 0, 1, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZModel:\n",
    "    def __init__(self, eps=1e-6): # threshold,\n",
    "        self.eps = eps\n",
    "        #self.threshold = threshold\n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "    def predict(self, X, y=None):\n",
    "        nX = (X - np.mean(self.X, axis=0)) / np.std( np.where(X > self.eps, X, self.eps), axis=0 )\n",
    "        return (nX ** 2).sum(axis=1).astype(int)\n",
    "        #novel = np.where( np.absolute(nX) > self.threshold, 1, 0)\n",
    "        #return novel.any(axis=1).astype(int)\n",
    "\n",
    "    \n",
    "class ZScore(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500): # threshold,\n",
    "        \n",
    "        alg = ZModel()\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MZModel:\n",
    "    def __init__(self, eps=1e-6): #  threshold,\n",
    "        self.eps = eps\n",
    "        #self.threshold = threshold\n",
    "        # assert threshold > 0\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n",
    "        return (nX ** 2).sum(axis=1).astype(int)\n",
    "        #novel = np.where( np.absolute(nX) > self.threshold, 1, 0)\n",
    "        #return novel.any(axis=1).astype(int)\n",
    "\n",
    "    \n",
    "class MZScore(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples=500):\n",
    "        \n",
    "        alg = MZModel()\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOFWrapper:\n",
    "    def __init__(self, n_neighbors, contamination):\n",
    "        self.alg = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True,\n",
    "                                      contamination=contamination, n_jobs=4, algorithm='kd_tree')\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.alg.fit(X)\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        #return np.where( self.alg.decision_function(X) > self.threshold, 0, 1)\n",
    "        return -self.alg.decision_function(X)\n",
    "\n",
    "\n",
    "class LOF(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, n_neighbors=20,\n",
    "                 contamination=0.1):\n",
    "        \n",
    "        alg = LOFWrapper(n_neighbors, contamination)\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnWrapper:\n",
    "    def __init__(self, algorithm, n_neighbors):\n",
    "        self.algorithm = algorithm\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        res = []\n",
    "        for obj in X:\n",
    "            dist, ind = self.get_neighbors([obj])\n",
    "            int_lof = dist[0, -1] # radius of sphere\n",
    "\n",
    "            dist, ind = self.get_neighbors( [self.X[i] for i in ind[0]] )\n",
    "            ext_lof = np.mean( dist[:, -1] )\n",
    "            \n",
    "            res.append( -(1 + ext_lof) / (1 + int_lof) )\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def get_neighbors(self, X):\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.algorithm == 'kd_tree':\n",
    "\n",
    "            tree = KDTree(self.X)\n",
    "            dist, ind = tree.query(X, k=self.n_neighbors)\n",
    "            return dist, ind\n",
    "\n",
    "        else:\n",
    "            raise Exception('unknown algorithm')\n",
    "        \n",
    "\n",
    "\n",
    "class Knn(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, n_neighbors=5, max_samples = 5000,\n",
    "                    algorithm='kd_tree', metric='euclidean'):\n",
    "        \n",
    "        alg = KnnWrapper(algorithm=algorithm, n_neighbors=n_neighbors)\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        if not isinstance(predict, list):\n",
    "            return predict.tolist()\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMWrapper:\n",
    "    def __init__(self, nu):\n",
    "        self.alg = OneClassSVM(nu=nu)\n",
    "    def fit(self, X):\n",
    "        self.alg.fit(X)\n",
    "    def predict(self, X):\n",
    "        return -self.alg.decision_function(X)\n",
    "\n",
    "class SVM(NovelPredictor):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 1000, nu=0.5):\n",
    "        \n",
    "        alg = SVMWrapper(nu)\n",
    "        super().__init__(alg, n_start, max_samples)\n",
    "        \n",
    "    def decision_function(self, predict):\n",
    "        return predict.tolist()\n",
    "        #return np.where(predict == -1, 1, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамблирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vote():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def average_voting(self, targets):\n",
    "        return np.mean(targets, axis=0).tolist()\n",
    "    \n",
    "    def max_voting(self, targets):\n",
    "        return np.max(targets, axis=0).tolist()\n",
    "    \n",
    "    def weighted_average_voting(self, targets, weights):\n",
    "        assert abs( sum(weights) - 1 ) < 1e-8\n",
    "        return np.sum( np.array(targets) * np.asarray(weights)[:, None], axis=0 )\n",
    "    \n",
    "    def average_max_voting(self, targets, count):\n",
    "        return np.mean( np.partition(targets, -np.arange(1, count + 1), axis=0)[-count:] ).tolist()\n",
    "    \n",
    "    \n",
    "def get_roc_auc(y_true, y_pred):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        res = 0.5\n",
    "    \n",
    "    return res\n",
    "    \n",
    "    \n",
    "def reset_pipeline(pipeline):\n",
    "    for pl_num, (pp, vectorizer, svd, alg) in enumerate(pipeline):\n",
    "        alg.reset()\n",
    "        \n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-np.asarray(x)))).tolist()\n",
    "    \n",
    "    \n",
    "def data_ensemble_predict(pipeline, batch_size, voting, n_start, verbose=1):\n",
    "    \"\"\"\n",
    "    pipeline[i]: [pp, vectorizer, svd, algorithm]\n",
    "    voting: voting function\n",
    "    \n",
    "    return: answers, targets\n",
    "    \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    targets = []\n",
    "\n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "\n",
    "        y_pred = []\n",
    "        for pl_num, (pp, vectorizer, svd, alg) in enumerate(pipeline):\n",
    "            batch = pp.preprocess1(batch)\n",
    "            vectors = vectorizer.vectorize(batch)\n",
    "            if svd is not None:\n",
    "                vectors = svd.fit_transform(vectors) # don't use for doc-to-vec\n",
    "\n",
    "            y_pred.append( list(alg.predict(vectors)) )\n",
    "\n",
    "        answers += voting( y_pred )\n",
    "        targets += target\n",
    "\n",
    "    reset_pipeline(pipeline)\n",
    "        \n",
    "    return answers, targets\n",
    "        \n",
    "        \n",
    "\n",
    "def data_ensemble(pipeline, batch_size, vote, n_start, weights=None, repeat_count=1, verbose=1):\n",
    "    \"\"\"\n",
    "    pipeline[i]: [pp, vectorizer, svd, algorithm]\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1 / len(pipeline)] * len(pipeline)\n",
    "    \n",
    "    roc_auc = []\n",
    "    answers = []\n",
    "    ap, am, aw, amp  = [], [], [], []\n",
    "    \n",
    "    for iter_num in tqdm( range(repeat_count) ):\n",
    "        \n",
    "        targets = []\n",
    "        y_pred = [[] for i in range(len(pipeline))]\n",
    "        \n",
    "        for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "            for pl_num, (pp, vectorizer, svd, alg) in enumerate(pipeline):\n",
    "                batch = pp.preprocess1(batch)\n",
    "                vectors = vectorizer.vectorize(batch)\n",
    "                if svd is not None:\n",
    "                    vectors = svd.fit_transform(vectors) # don't use for doc-to-vec\n",
    "                \n",
    "                y_pred[pl_num] += list(alg.predict(vectors))\n",
    "            \n",
    "            targets += target\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred[i] = sigmoid( y_pred[i][n_start:] )\n",
    "        \n",
    "        ap = vote.average_voting( y_pred )\n",
    "        mp = vote.max_voting( y_pred )\n",
    "        wap = vote.weighted_average_voting( y_pred, weights )\n",
    "        amp = vote.average_max_voting( y_pred, count=3 )\n",
    "\n",
    "        targets = targets[n_start:]\n",
    "        \n",
    "        \n",
    "        roc_auc.append( [get_roc_auc(targets, ap),\n",
    "                          get_roc_auc(targets, mp),\n",
    "                          get_roc_auc(targets, wap),\n",
    "                          get_roc_auc(targets, amp)])\n",
    "        \n",
    "        reset_pipeline(pipeline)\n",
    "        \n",
    "        if verbose:\n",
    "            print('iter num: {}'.format(iter_num))\n",
    "            print( 'roc auc:', roc_auc[-1] )\n",
    "            print()\n",
    "    \n",
    "    if verbose:\n",
    "        print('ROC AUC: AP | MP | WAP | AMP')\n",
    "        print('ROC AUC: ', np.mean(roc_auc, axis=0))\n",
    "        print()\n",
    "        \n",
    "    return np.mean(roc_auc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = Vote()\n",
    "batch_size = 500\n",
    "scores_iter = 3\n",
    "n_start = 100\n",
    "vectorization='tf-idf'\n",
    "\n",
    "weighted_algorithms = [ \n",
    "    IQRModel(n_start=n_start, max_samples=500),\n",
    "    Dbscan(n_start=n_start, max_samples=500, eps=0.1, min_samples=15),\n",
    "    Ellipse(n_start=n_start, max_samples=500, contamination=0.75), # \n",
    "    LOF(n_start=n_start, max_samples=500, contamination=0.01, n_neighbors=5), # \n",
    "    Knn(n_start=n_start, max_samples=500, n_neighbors=5),\n",
    "    SVM(n_start=n_start, max_samples=500, nu=0.0001), # \n",
    "    LOF(n_start=n_start, max_samples=500, contamination=0.0001, n_neighbors=5), \n",
    "    SVM(n_start=n_start, max_samples=500, nu=0.1), \n",
    "]\n",
    "    \n",
    "#weights = [0.15, 0.15, 0.4, 0.15, 0.15]\n",
    "weights = [0.2, 0.4, 0.4]\n",
    "    \n",
    "pipeline = [\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, random_state=5, max_iter=50), weighted_algorithms[0] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         NMF(n_components=5, init='random', max_iter=1000, random_state=0), weighted_algorithms[0] ],\n",
    "#     [ Preprocessor(), Vectorizer('doc-to-vec', vocab_corpus),\n",
    "#         TruncatedSVD(n_components=5, n_iter=100, random_state=0), weighted_algorithms[7] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=5, n_jobs=-1, random_state=5), weighted_algorithms[1] ],\n",
    "    [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "        NMF(n_components=10, init='random', max_iter=1000, random_state=0), weighted_algorithms[2] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, random_state=42), weighted_algorithms[3] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, random_state=5, max_iter=500), weighted_algorithms[3] ],\n",
    "    [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "        NMF(n_components=5, init='random', max_iter=1000, random_state=0), weighted_algorithms[3] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, n_jobs=-1, random_state=42), weighted_algorithms[3] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, n_jobs=-1, random_state=5), weighted_algorithms[4] ],\n",
    "    [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "        NMF(n_components=5, init='random', max_iter=1000, random_state=0), weighted_algorithms[5] ],\n",
    "    \n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         NMF(n_components=10, init='random', max_iter=1000, random_state=0), weighted_algorithms[6] ],\n",
    "    \n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         NMF(n_components=5, init='random', max_iter=1000, random_state=0), weighted_algorithms[7] ],\n",
    "    \n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=6, n_jobs=-1, random_state=5), weighted_algorithms[6] ],\n",
    "#     [ Preprocessor(), Vectorizer('tf-idf', vocab_corpus),\n",
    "#         LDA(n_components=10, n_jobs=-1, random_state=5), weighted_algorithms[7] ],\n",
    "]\n",
    "\n",
    "# weights = [w[1] for w in weighted_algorithms] # [0.25,0.25,0.25,0.25]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca789c56a16a43ec8dfdfac4adf728c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27109bdeca334fcd8b3a0939b3fe7ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter num: 0\n",
      "roc auc: [0.8179760029225747, 0.645848345777108, 0.8271700116446332, 0.5]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-10dcc5f6f243>:34: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-np.asarray(x)))).tolist()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f18d6f952484b289856cc327a380d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter num: 1\n",
      "roc auc: [0.8185984199831039, 0.6466198598077494, 0.8276376235815238, 0.5]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-10dcc5f6f243>:34: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-np.asarray(x)))).tolist()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5313e18dd443b6a7279b1d0939fbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter num: 2\n",
      "roc auc: [0.8180616252254721, 0.6461298719090349, 0.8271976391077014, 0.5]\n",
      "\n",
      "\n",
      "ROC AUC: AP | MP | WAP | AMP\n",
      "ROC AUC:  [0.81821202 0.64619936 0.82733509 0.5       ]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-10dcc5f6f243>:34: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-np.asarray(x)))).tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81821202, 0.64619936, 0.82733509, 0.5       ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ensemble(pipeline=pipeline, batch_size=batch_size, vote=vote,\n",
    "              n_start=n_start, repeat_count=scores_iter, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
