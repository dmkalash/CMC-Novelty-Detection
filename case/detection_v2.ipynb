{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('twitter_samples')\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "\n",
    "\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier, KDTree\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения исследования, скачаем весь датасет, и будем собирать батчи из него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/poems_150_5_5.csv', low_memory=False)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3518, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16202387720295622"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( df.novel ) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generator(batch_size=10):\n",
    "    \n",
    "    # df = pd.read_csv('twitter_dataset.csv')\n",
    "    for i in range(0, df.shape[0], batch_size):\n",
    "        stream_batch = df.iloc[i : min(i + batch_size, df.shape[0])]\n",
    "        yield stream_batch['content'].tolist(), stream_batch['novel'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The burden of hard hitting. Slug away Like Honus Wagner or like Tyrus Cobb. Else fandom shouteth: “Who said you could play? Back to the jasper league, you minor slob!” Swat, hit, connect, line out, get on the job. Else you shall feel the brunt of fandom’s ire Biff, bang it, clout it, hit it on the knob— This is the end of every fan’s desire. The burden of good pitching. Curved or straight. Or in or out, or haply up or down, To puzzle him that standeth by the plate, To lessen, so to speak, his bat-renoun: Like Christy Mathewson or Miner Brown, So pitch that every man can but admire And offer you the freedom of the town— This is the end of every fan’s desire. The burden of loud cheering. O the sounds! The tumult and the shouting from the throats Of forty thousand at the Polo Grounds Sitting, ay, standing sans their hats and coats. A mighty cheer that possibly denotes That Cub or Pirate fat is in the fire; Or, as H. James would say, We’ve got their goats— This is the end of every fan’s desire. The burden of a pennant. O the hope, The tenuous hope, the hope that’s half a fear, The lengthy season and the boundless dope, And the bromidic; “Wait until next year.” O dread disgrace of trailing in the rear, O Piece of Bunting, flying high and higher That next October it shall flutter here: This is the end of every fan’s desire. ENVOY Ah, Fans, let not the Quarry but the Chase Be that to which most fondly we aspire! For us not Stake, but Game; not Goal, but Race— THIS is the end of every fan’s desire.', '\\nThese are the saddest of possible words: “Tinker to Evers to Chance.” Trio of bear cubs, and fleeter than birds, Tinker and Evers and Chance. Ruthlessly pricking our gonfalon bubble, Making a Giant hit into a double— Words that are heavy with nothing but trouble: “Tinker to Evers to Chance.”', '\\nMy desk is cleared of the litter of ages; Before me glitter the fair white pages; My fountain pen is clean and filled, And the noise of the office has long been stilled. Roget’s Thesaurus is at my hand, And I’m ready to do some work that’s grand, Dignified, eminent, great, momentous, Memorable, worthy of note, portentous, Beautiful, paramount, vital, prime, Stirring, eventful, august, sublime. For this is the way, I have read and heard, That authors look for the fitting word. All of the proud ingredients mine To build, like Marlowe, the mighty line. But never a line from my new-filled pen That couldn’t be done by a child of ten. Oh, how did Shelley and how did Keats Weave magic words on the fair white sheets Under conditions that, were they mine, I couldn’t bear? And I’d just resign. Yet Milton wrote passable literature Under conditions I couldn’t endure. Coleridge and Chatterton did their stuff Over a road that I’d christen rough. Wordsworth and—soft!—could it be that they Waited until they had something to say?'], [1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in stream_generator(3):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые стоп-слова удалим. Применим в одном варианте лемматизацию, в другом - стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.methods = {\n",
    "            'lemm' : self.lemmatization,\n",
    "            'stem' : self.stemming\n",
    "        }\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.swords = set( stopwords.words(\"english\") )\n",
    "     \n",
    "    def preprocess1(self, content_batch, standard='lemm'):\n",
    "        # stream_batch = ([content], [novel]), content = list(string), novel = list\n",
    "    \n",
    "        preprocessed_batch = []\n",
    "        for doc in content_batch:\n",
    "            doc = doc.lower()\n",
    "            # doc = self.delete_tags(doc)\n",
    "            # doc = self.delete_links(doc)\n",
    "            doc = self.delete_garbage(doc)\n",
    "            tokens = self.get_tokens(doc)\n",
    "            tokens = self.methods[standard](tokens)\n",
    "            tokens = self.delete_stop_words(tokens)\n",
    "            preprocessed_batch.append( ' '.join(tokens) )\n",
    "            \n",
    "        return preprocessed_batch\n",
    "    \n",
    "    \n",
    "    def delete_tags(self, doc):\n",
    "        doc = re.sub(r'^@[\\w]*', ' ', doc) \n",
    "        doc = re.sub(r'\\s@[\\w]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_links(self, doc):\n",
    "        doc = re.sub(r'http\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'https\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'ftp\\:\\/\\/[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        doc = re.sub(r'www\\.[\\w\\-&\\./?=\\+;@#%]*', ' ', doc)\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_garbage(self, doc):\n",
    "        doc = re.sub(r'\\s+', ' ', doc)\n",
    "        doc = re.sub(r\"[^a-zA-Z0-9\\s\\']*\", '', doc) # TODO: посмотреть что оставляет CountVectorizer\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def delete_stop_words(self, tokens): # TODO: create own sw list\n",
    "        # new_tokens = []\n",
    "        return list( filter(lambda sword: sword not in self.swords, tokens) )\n",
    "#         for sword in swords:\n",
    "#             if sword not in stop_words:\n",
    "#                 new_tokens.append(sword)\n",
    "#         return new_tokens\n",
    "    \n",
    "    \n",
    "    def get_tokens(self, doc):\n",
    "        return list(map(lambda token: token.lower(), doc.split()))\n",
    "    \n",
    "    \n",
    "    def lemmatization(self, tokens):\n",
    "        return list(map(lambda token: self.lemmatizer.lemmatize(token), tokens))\n",
    "    \n",
    "    \n",
    "    def stemming(self, tokens):\n",
    "        return list( map(lambda token: self.stemmer.stem(token), tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем корпус из классических произведений, и создадим из него фиксированный словарь для векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print( gutenberg.fileids() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    vocab_corpus = []\n",
    "    \n",
    "    def normalize(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9]', '', s.lower())\n",
    "        \n",
    "    for filename in gutenberg.fileids():\n",
    "        #vocab_corpus += list(gutenberg.words(filename))\n",
    "        words = set(map(normalize, gutenberg.words(filename)))\n",
    "        vocab_corpus += list( filter(lambda s: len(s) > 0, words) )\n",
    "    \n",
    "    return vocab_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_corpus = create_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boiling',\n",
       " 'transplantation',\n",
       " 'irish',\n",
       " 'unmerited',\n",
       " 'travelled',\n",
       " 'persuadable',\n",
       " 'odd',\n",
       " 'appreciate',\n",
       " 'gaieties',\n",
       " 'modes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121719\n",
      "boiling\n"
     ]
    }
   ],
   "source": [
    "print( len(vocab_corpus) )\n",
    "print( vocab_corpus[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс, в котором будут реализованы основные методы векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self, method, train_corpus):\n",
    "        self.methods = {\n",
    "            'one-hot' : self.one_hot_vectorizer,\n",
    "            'count' : self.count_vectorizer,\n",
    "            'tf-idf' : self.tfidf_vectorizer,\n",
    "            #'n-gramms' : self.n_gramms_vectorizer,\n",
    "            'doc-to-vec' : self.doc_to_vec_vectorizer\n",
    "        }\n",
    "        if method not in self.methods:\n",
    "            raise Exception('Wrong method: {}'.format(method))\n",
    "        \n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.train_corpus = Preprocessor().preprocess1( train_corpus )\n",
    "        \n",
    "    \n",
    "    def vectorize(self, batch, **args):\n",
    "        return self.methods[self.method](batch, **args)\n",
    "        \n",
    "    \n",
    "    def one_hot_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(binary=True, **args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def count_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = CountVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "    \n",
    "    \n",
    "    def tfidf_vectorizer(self, batch, **args):\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = TfidfVectorizer(**args)\n",
    "            self.model.fit(self.train_corpus)\n",
    "            \n",
    "        return self.model.transform(batch)\n",
    "        \n",
    "        \n",
    "    def doc_to_vec_vectorizer(self, batch, **args):\n",
    "        \n",
    "        def extract_tokens(train = False):\n",
    "            if train:\n",
    "                for i, doc in enumerate(self.train_corpus):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])    \n",
    "                    \n",
    "            else:\n",
    "                for i, doc in enumerate(batch):\n",
    "                    tokens = Preprocessor().get_tokens(doc)\n",
    "                    yield tokens\n",
    "\n",
    "        \n",
    "        if self.model is None:\n",
    "            # vocab = list( extract_tokens(train=True) )\n",
    "            vocab = list( extract_tokens(train=True) )\n",
    "            self.model = gensim.models.doc2vec.Doc2Vec(min_count=1, vector_size=20)\n",
    "            # self.model.build_vocab(train_corpus, update = True)\n",
    "            # self.model.build_vocab(self.train_corpus)\n",
    "            self.model.build_vocab( vocab )\n",
    "            self.model.train(vocab, total_examples=self.model.corpus_count, epochs=5, **args)\n",
    "        \n",
    "        return np.array( list( map( lambda token: self.model.infer_vector(token), extract_tokens() ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализируем на плоскости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "v1 = Vectorizer('tf-idf', vocab_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 500\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 3518 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 3518 samples in 1.848s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3518\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3518\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3518\n",
      "[t-SNE] Computed conditional probabilities for sample 3518 / 3518\n",
      "[t-SNE] Mean sigma: 0.430557\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 167.123077\n",
      "[t-SNE] KL divergence after 500 iterations: 7.449514\n"
     ]
    }
   ],
   "source": [
    "X = df['content'].tolist()\n",
    "X = pp.preprocess1(X)\n",
    "X = v1.vectorize(X)\n",
    "Xe = TSNE(n_components=2, n_iter=n_iter, n_jobs=n_jobs, verbose=1).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clusters(Xe):\n",
    "    plt.figure(figsize=(14,9))\n",
    "    types = df['author_id'].nunique()\n",
    "    ind = np.random.choice(types, 100, replace=False)\n",
    "    \n",
    "    for i in ind:\n",
    "        mask = (df['author_id'] == i).astype(dtype=int).to_numpy()\n",
    "        Xi = Xe[np.where(mask)]\n",
    "        plt.scatter(Xi[:, 0], Xi[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_clusters(Xe)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной визуалиции кластеры разных тем никак не отделены друг от друга. Посмотрим, как с этим справятся алгоритмы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты замеров есть в таблице:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1a6M76eS8L-WTGKVI-xRajut2P-X70j-bv5s_7WZv3ZE/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alg(pp, alg, svd_compenents, n_iter, batch_size, vectorization, eps):\n",
    "\n",
    "    svd = TruncatedSVD(n_components=svd_compenents, n_iter=n_iter, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    answers = []\n",
    "    targets = []\n",
    "\n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "        vectors = v1.vectorize(batch)\n",
    "        if vectorization != 'doc-to-vec':\n",
    "            vectors = svd.fit_transform(vectors)\n",
    "\n",
    "        #scores.append( list(alg.check_novelty(vectors)) ) #\n",
    "        \n",
    "        answers += list(alg.check_novelty(vectors)) #\n",
    "        #answers += [int(score < alg.threshold) for score in scores[-1]] #\n",
    "        targets += target\n",
    "        \n",
    "        # scores.append( list(alg.check_novelty(vectors)) ) #\n",
    "    \n",
    "    print( svd_compenents, n_iter, batch_size, vectorization, eps)\n",
    "    print( accuracy_score(answers, targets) )\n",
    "    print( f1_score(answers, targets) )\n",
    "    print( roc_auc_score(answers, targets) )\n",
    "    print()\n",
    "    \n",
    "    return accuracy_score(answers, targets), f1_score(answers, targets), roc_auc_score(answers, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQR:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X.copy()\n",
    "    def predict(self, X, y=None):\n",
    "        q75, q25 = np.percentile(self.X, [75, 25], axis=0)\n",
    "        iqr = q75 - q25\n",
    "        scores = np.where( (X > q25 - 1.5 * iqr).astype(int) * (X < q75 + 1.5 * iqr).astype(int) == 1, 0, 1)\n",
    "        scores = np.sum( scores, axis=1 )\n",
    "        return np.where(scores > 0, 1, 0)\n",
    "\n",
    "\n",
    "class IQRModel(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, eps=0.4, min_samples=10):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.alg = IQR()\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count].copy()\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return self.alg.predict(X)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ].copy()\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos].copy()\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef19891a1a34ad99d2ba1c379e0b692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.6776577600909608\n",
      "0.1782608695652174\n",
      "0.49339269106625094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for contamination in [0.0001, 0.001, 0.01, 0.1, ]:\n",
    "    try:\n",
    "        alg = IQRModel(max_samples=500)\n",
    "        \n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=10, n_iter=5,\n",
    "                                   batch_size=100, vectorization='tf-idf', eps=0)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    \n",
    "    m_acc.append(acc)\n",
    "    m_f1.append(f1)\n",
    "    m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dbscan(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, eps=0.4, min_samples=10):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.alg = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit( np.concatenate( (self.data, X[start_count:]), axis=0) )\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return np.where( self.alg.labels_[self.data.shape[0]:] < 0, 1, 0)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ]\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos]\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36337cf3c2174708a106abbb879cffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pp = Preprocessor()\n",
    "# alg = Dbscan(max_samples=500, eps = 0.4, min_samples=1)\n",
    "# svd = TruncatedSVD(n_components=10, n_iter=5, random_state=42)\n",
    "# answers = []\n",
    "# targets = []\n",
    "\n",
    "# for batch, target in tqdm( stream_generator(100) ):\n",
    "#         batch = pp.preprocess1(batch)\n",
    "#         v1 = Vectorizer('tf-idf', vocab_corpus)\n",
    "#         vectors = v1.vectorize(batch)\n",
    "#         vectors = svd.fit_transform(vectors)\n",
    "        \n",
    "#         answers += list(alg.check_novelty(vectors))\n",
    "#         targets += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc26442b82b49c29a1b8ffd77a5abd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.1665719158612848\n",
      "0.2796068796068796\n",
      "0.5535079365079365\n",
      "\n",
      "20 0.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e606d61c475b4f5aa362e96df27a3a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16543490619670267\n",
      "0.27933235149729996\n",
      "0.5454786366601435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for min_samples in [15, 20]:\n",
    "    for eps in [0.15]:\n",
    "        try:\n",
    "            alg = Dbscan(max_samples=500, eps = eps, min_samples=min_samples)\n",
    "            print(min_samples, eps)\n",
    "            acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=10, n_iter=5,\n",
    "                                       batch_size=100, vectorization='tf-idf', eps=0)\n",
    "\n",
    "        except Exception as err:\n",
    "            # raise err\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elliptical Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellipse(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, contamination=0.1):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.alg = EllipticEnvelope(contamination=contamination)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return np.where( self.alg.predict(X) < 0, 1, 0)\n",
    "        # return self.alg.decision_function(X)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ]\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos]\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95b5604b641490495688cb1136bdcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.8177942012507107\n",
      "0.055964653902798235\n",
      "0.506340474889728\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1feff25288c450b815d6faaa1122c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.8095508811824901\n",
      "0.056338028169014086\n",
      "0.4900194536073754\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1f2c5e71694bdaaf419ab384c93333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.778283115406481\n",
      "0.09302325581395349\n",
      "0.4868713412810324\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507c84a7d9124dafa48bd7283876a74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.7447413303013075\n",
      "0.13320463320463322\n",
      "0.4919570084205671\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e5909639b44154bf8cb0ea2fa384db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.6878908470722002\n",
      "0.1805970149253731\n",
      "0.49687564982324806\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4468f4a7519f457485ff6a630b7cb6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.6498010233086982\n",
      "0.18733509234828494\n",
      "0.4918491216187335\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280634724713426697dd9b14e4788a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.617964752700398\n",
      "0.2\n",
      "0.4922039148783335\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cd518eafa547cb88f8b718af6eeffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.587833996588971\n",
      "0.21366594360086769\n",
      "0.4942050246672095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for contamination in [0.001, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.35]:\n",
    "    try:\n",
    "        alg = Ellipse(max_samples=500, contamination=contamination)\n",
    "        \n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=10, n_iter=5,\n",
    "                                   batch_size=100, vectorization='tf-idf', eps=0)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZModel:\n",
    "    def __init__(self, threshold, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "    def predict(self, X, y=None):\n",
    "        nX = (X - np.mean(self.X, axis=0)) / np.std( np.where(X > self.eps, X, self.eps), axis=0 )\n",
    "        novel = np.where( np.absolute(nX) > self.threshold, 1, 0)\n",
    "        return novel.any(axis=1).astype(int)\n",
    "    \n",
    "\n",
    "class ZScore(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, threshold, n_start=100, max_samples=500):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.alg = ZModel(threshold)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return self.alg.predict(X)\n",
    "        # return self.alg.decision_function(X)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ]\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos]\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641d7d1a2eb3431c8f1ff8cc220d24fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9202501dcca8407ab02440bced7959cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776e7452ef67440db35ce3542c047e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803d81b395344b6fab9678c4b051dcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144978eda3314773a8fb6b076677cdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.1625923820352473\n",
      "0.27864838393731634\n",
      "0.45596186681844053\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9f5b786824c55920807fdaaaf4bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.251847640704946\n",
      "0.25775521714608013\n",
      "0.4725373715430703\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19102658cd444d1f9bef3d8303e63e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.7924957362137578\n",
      "0.09429280397022333\n",
      "0.4994603331990622\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3334e19748ee4df083c093f2c5d46173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6633940169a34360b40bb426c858fa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for threshold in [0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 100]:\n",
    "    try:\n",
    "        alg = ZScore(max_samples=500, threshold=threshold)\n",
    "        \n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=10, n_iter=5,\n",
    "                                   batch_size=100, vectorization='tf-idf', eps=0)\n",
    "    except Exception as err:\n",
    "        # raise err\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MZModel:\n",
    "    def __init__(self, threshold, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "        assert threshold > 0\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n",
    "        novel = np.where( np.absolute(nX) > self.threshold, 1, 0)\n",
    "        return novel.any(axis=1).astype(int)\n",
    "    \n",
    "class MZScore(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, threshold, n_start=100, max_samples=500):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.alg = MZModel(threshold)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count].copy()\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return self.alg.predict(X)\n",
    "        # return self.alg.decision_function(X)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ].copy()\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos].copy()\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2b3325ee59458ba65a57c1a6fb2aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91434a60d0a54c01aafe8af966dbf50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a427123958084ede9eefedcc354c29ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9c4e86b6164c0282f17743026fae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2821eba7024c9e84ee1dcccbe5cb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7219a404531f4034a457dacf1da8e5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e365d10f3d44158965cc975829ab1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e818f6583145e68a1bf076f25faacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08553bc7f0be4e72a47936a746e42131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-0cb7567efdea>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nX = (X - np.median(self.X, axis=0)) / mad( np.where(X > self.eps, X, self.eps), axis=0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 5 100 tf-idf 0\n",
      "0.16202387720295622\n",
      "0.2788649706457925\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for threshold in [0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 100]:\n",
    "    try:\n",
    "        alg = MZScore(max_samples=500, threshold=threshold)\n",
    "        \n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=10, n_iter=5,\n",
    "                                   batch_size=100, vectorization='tf-idf', eps=0)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOF(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 500, n_neighbors=20,\n",
    "                 contamination=0.1, threshold=0):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.threshold = threshold\n",
    "        self.alg = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True,\n",
    "                                      contamination=contamination, n_jobs=4, algorithm='kd_tree')\n",
    "        \n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.get_scores(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "    \n",
    "    \n",
    "    def get_scores(self, X):\n",
    "        return np.where( self.alg.decision_function(X) > self.threshold, 0, 1)\n",
    "        # return self.alg.decision_function(X)\n",
    "    \n",
    "    \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ]\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos]\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ac106c66640f29221b1b8ebfb9524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (18,18) into shape (18,20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-35e40f462fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    batch_size=100, vectorization='tf-idf', eps=0)\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(err)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-35e40f462fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0malg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=svd_components, n_iter=5,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                    batch_size=100, vectorization='tf-idf', eps=0)\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5a69c5dfc4b7>\u001b[0m in \u001b[0;36mrun_alg\u001b[0;34m(pp, alg, svd_compenents, n_iter, batch_size, vectorization, eps)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#scores.append( list(alg.check_novelty(vectors)) ) #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0manswers\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_novelty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#answers += [int(score < alg.threshold) for score in scores[-1]] #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f94f460a4925>\u001b[0m in \u001b[0;36mcheck_novelty\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_normal_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f94f460a4925>\u001b[0m in \u001b[0;36mupdate_storage\u001b[0;34m(self, X, start_count)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_count\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_count\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_count\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (18,18) into shape (18,20)"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for svd_components in [20, 50]: # 2, 5, 10, 15, \n",
    "    try:\n",
    "        alg = LOF(max_samples=500, threshold=0)\n",
    "        \n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=svd_components, n_iter=5,\n",
    "                                   batch_size=100, vectorization='tf-idf', eps=0)\n",
    "    except Exception as err:\n",
    "        raise err\n",
    "        #print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве обратной меры новизны будем использовать отношение средней плотности k ближайших к объекту соседей и локальной плотности самого объекта. Если это отношение будет достаточно близко к нулю, то можно воспринимать объект как аномальный.\n",
    "\n",
    "Локальную плотность считаем так - находим минимальный радиус сферы, внутри которой находятся k ближайших соседей для данного объекта(считая сам объект).\n",
    "\n",
    "Но нет, лучше зафиксируем k, и будем смотреть r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "\n",
    "- Считываем достаточное количество примеров из потока для обучения: n_start = 200. \n",
    "- Далее считываем по батчу данных размера n_batch\n",
    "- Для каждого объекта в батче считаем локальную плотность, и заодно ищем индексы k_nbr = 5 ближайших соседей \n",
    "- Для каждого из k_nbr ближайших соседей считаем локальную плотность\n",
    "- Усредняем, делим, получаем обратную меру новизны\n",
    "- Сравниваем с порогом(который сначала нужно подобрать)\n",
    "- В конце обработки элемента добавляем его в известную выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ро = k / r^dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knn(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, n_neighbors=5, max_samples = 5000,\n",
    "                 default_normal_score=1, eps=0.5683, algorithm='kd_tree', metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        self.metric = metric \n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = default_normal_score\n",
    "        self.threshold = eps\n",
    "        \n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype) \n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "        \n",
    "        for x in X[start_count: ]:\n",
    "            score = self.get_novel_score(x)\n",
    "            if self.pos >= self.data.shape[0]:\n",
    "                self.data = np.append(self.data, [x], axis=0)\n",
    "            self.pos = (self.pos + 1) % self.max_samples\n",
    "            yield score\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_novel_score(self, obj): # normal score\n",
    "\n",
    "        dist, ind = self.get_neighbors([obj])\n",
    "        int_lof = dist[0, -1] # radius of sphere\n",
    "        \n",
    "        dist, ind = self.get_neighbors( [self.data[i] for i in ind[0]] )\n",
    "        ext_lof = np.mean( dist[:, -1] )\n",
    "        \n",
    "        return int( np.where((1 + ext_lof) / (1 + int_lof) < self.threshold, 1, 0) )\n",
    "    \n",
    "    \n",
    "    def get_neighbors(self, X):\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.algorithm == 'kd_tree':\n",
    "\n",
    "            tree = KDTree(self.data)\n",
    "            dist, ind = tree.query(X, k=self.n_neighbors)\n",
    "            return dist, ind\n",
    "\n",
    "        else:\n",
    "            raise Exception('unknown algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddb846b9bad466ebfe7a401f98edf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 7 300 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47a8ac8646a4d02885f0496de7e9268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 7 300 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3f65cb1e9b498794efcad066f8f2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 7 300 tf-idf 0.8444444444444446\n",
      "0.8186469584991473\n",
      "0.05621301775147929\n",
      "0.5088782100909112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.68888, 0.76666, 0.8444444444444446]:\n",
    "    try:\n",
    "        alg = Knn(max_samples=2000, eps=eps)\n",
    "        acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=5, n_iter=7,\n",
    "                                   batch_size=300, vectorization='tf-idf', eps=eps)\n",
    "    except Exception as err:\n",
    "        raise err\n",
    "        #print(err)\n",
    "    m_acc.append(acc)\n",
    "    m_f1.append(f1)\n",
    "    m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4401ce67ccd24468a8172e8c269dae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 2000 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b248da70b14468e8b04d4eec2daa214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 2000 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d13effdce045d3848aa7889b40c5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 2000 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a86bafd95f942429dc53cde80334962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 500 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7d8b9849fd4e549b0dec0228d31ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 500 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c74f93409a34c49b341d08b2772abbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 500 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48142e29ffae49e48015014f785d9981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 300 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a82fa469404a1ab5a5229c3390102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 300 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b9721bd1e34b069d7277eaf79d7b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 300 tf-idf 0.55\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39263d8bfeb04d9c888add8d9168c0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 2000 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69f368e795343dc82b27b8518dc7a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 2000 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c071e5782f4bbfb81414df464067d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 2000 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d5d18f1d3944a48240e6654ed5c35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 500 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dafc81a586a4e68b3010cb9f5a7fcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 500 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b64f146290432ebc36af0343172730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 500 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174256c1f7764ae68dff002cb9ef7f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 300 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fa87c525934cdc96eb5537df44371c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 300 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46bc1959e4f43e5affcac91465637b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 300 tf-idf 0.68888\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d7f223c54848948ff64165a33f732f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 2000 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c851ee56654472bc462933c9eb8f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 2000 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13710674a3b7449f811b1c8f60f0e922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 2000 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c999ec209d9e41cab0e2be7fddd70c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 500 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee620b2b0824a8eaaeab7a7e921676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 500 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55f03613b8947e5bc54f21cd0b8f33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 500 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b8a9f150624ed9a34ca1efc75a6fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 300 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6419cfed18a45e8902c3808889f402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 300 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84f09df89444cf97800ef2abe200d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 300 tf-idf 0.76666\n",
      "0.8197839681637293\n",
      "0.053731343283582096\n",
      "0.5092510239906378\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6e69a835e54fe086646dc3afeb62ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 2000 tf-idf 0.8444444444444446\n",
      "0.8180784536668562\n",
      "0.05325443786982248\n",
      "0.5040146873410162\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8d5a39144d421f94201eec946339b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 2000 tf-idf 0.8444444444444446\n",
      "0.8180784536668562\n",
      "0.05325443786982248\n",
      "0.5040146873410162\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ff1d77ad814f019c94c6a07ef7f13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 50 2000 tf-idf 0.8444444444444446\n",
      "0.8180784536668562\n",
      "0.05325443786982248\n",
      "0.5040146873410162\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23f969cb78447f2a7c67e3785766dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 10 500 tf-idf 0.8444444444444446\n",
      "0.8172256964184196\n",
      "0.058565153733528545\n",
      "0.5077319922550129\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de82a0f0f18e4b089d3ddaf9893634f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 25 500 tf-idf 0.8444444444444446\n",
      "0.8172256964184196\n",
      "0.058565153733528545\n",
      "0.5077319922550129\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100773ed18e6451099594e803302d4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6cb3001a01c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0malg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=5, n_iter=n_iter,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                            batch_size=batch_size, vectorization='tf-idf', eps=eps)\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5a69c5dfc4b7>\u001b[0m in \u001b[0;36mrun_alg\u001b[0;34m(pp, alg, svd_compenents, n_iter, batch_size, vectorization, eps)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvectorization\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'doc-to-vec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-858942d83bc9>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, batch, **args)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-858942d83bc9>\u001b[0m in \u001b[0;36mtfidf_vectorizer\u001b[0;34m(self, batch, **args)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,\n\u001b[0m\u001b[1;32m     76\u001b[0m              preprocessor=None, decoder=None, stop_words=None):\n\u001b[1;32m     77\u001b[0m     \"\"\"Chain together an optional series of text processing steps to go from\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "alg = Knn(max_samples=2000, )\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.55, 0.68888, 0.76666, 0.8444444444444446]:\n",
    "    for batch_size in [2000, 500, 300]:\n",
    "        for n_iter in [10, 25, 50]:\n",
    "            try:\n",
    "                alg = Knn(max_samples=2000, eps=eps)\n",
    "                acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=5, n_iter=n_iter,\n",
    "                                           batch_size=batch_size, vectorization='tf-idf', eps=eps)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            m_acc.append(acc)\n",
    "            m_f1.append(f1)\n",
    "            m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor()\n",
    "alg = Knn(max_samples=2000, )\n",
    "\n",
    "m_acc = []\n",
    "m_f1 = []\n",
    "m_roc_auc = []\n",
    "\n",
    "for eps in [0.68888, 0.76666, 0.8444444444444446]:\n",
    "    for batch_size in [500, 300]:\n",
    "        for svd_compenents in [3, 10, 20, 30]:\n",
    "            try:\n",
    "                alg = Knn(max_samples=2000, eps=eps)\n",
    "                acc, f1, roc_auc = run_alg(pp, alg, svd_compenents=svd_compenents, n_iter=10,\n",
    "                                           batch_size=batch_size, vectorization='tf-idf', eps=eps)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            m_acc.append(acc)\n",
    "            m_f1.append(f1)\n",
    "            m_roc_auc.append(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_start=100, max_samples = 1000, nu=0.05):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        self.default_normal_score = 1\n",
    "        self.alg = OneClassSVM(nu=nu)\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "    \n",
    "    \n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')\n",
    "        \n",
    "        start_count = 0\n",
    "        \n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.n_for_start, X.shape[0])\n",
    "            \n",
    "            self.data = X[:start_count]\n",
    "            \n",
    "            pos = start_count % self.max_samples\n",
    "            self.initialized = True\n",
    "            \n",
    "            for i in range(start_count):\n",
    "                yield self.default_normal_score\n",
    "                \n",
    "        self.update_storage(X, start_count)\n",
    "        self.alg.fit(self.data)\n",
    "        \n",
    "        if start_count < X.shape[0]:\n",
    "            for target in self.alg.predict(X[start_count:]):\n",
    "                yield target\n",
    "            \n",
    "            \n",
    "    def update_storage(self, X, start_count):\n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ]\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos]\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aedcb44307438489e4c43460b80f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1802160318362706\n",
      "0.2768304914744233\n",
      "0.4981182603727773\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee3f89f4d204575b308cf2728471d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5911028993746447\n",
      "0.22095857026807472\n",
      "0.49704438334642576\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf02d69c6d840159d1da6812e86862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5358158044343377\n",
      "0.23727230266230734\n",
      "0.49943524006760454\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd515edc28e4407b337725bf8eb3b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5493177942012507\n",
      "0.23424293648877087\n",
      "0.49935430502987455\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dea40874ad4c669aa8934ebbff2949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5216600341102899\n",
      "0.24061371841155235\n",
      "0.49990442524221\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cea4e12aca48a6bb61c9942f700a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5253458404396437\n",
      "0.23734490370708686\n",
      "0.4973156545819546\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bab9dd446c4722b44f2cc969515047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5058474782749939\n",
      "0.2438327223016218\n",
      "0.5001532834800706\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839e01fb4c9d43e58fa9daae3dd7dee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5111569073337123\n",
      "0.24148197155143897\n",
      "0.4986967078483182\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8864134118c42619dc22f955e8d5dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.49741014465289624\n",
      "0.24522126832044777\n",
      "0.5000271105927037\n",
      "\n",
      "CPU times: user 3min 34s, sys: 23 s, total: 3min 57s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 100\n",
    "vectorization = 'tf-idf'\n",
    "svd = TruncatedSVD(n_components=10, n_iter=5, random_state=42)\n",
    "v1 = Vectorizer(vectorization, vocab_corpus)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pp = Preprocessor()\n",
    "\n",
    "for nu in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]:\n",
    "    alg = SVM(nu=nu)\n",
    "    \n",
    "    for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "        batch = pp.preprocess1(batch)\n",
    "        vectors = v1.vectorize(batch)\n",
    "        vectors = svd.fit_transform(vectors)\n",
    "        y_pred += list( alg.check_novelty(vectors) )\n",
    "        y_true += target\n",
    "\n",
    "    y_pred = np.where(np.array(y_pred) == 1, 0, 1).tolist()\n",
    "\n",
    "    print( accuracy_score(y_true, y_pred) )\n",
    "    print( f1_score(y_true, y_pred) )\n",
    "    try:\n",
    "        print( roc_auc_score(y_true, y_pred) )\n",
    "    except:\n",
    "        print('roc auc failed')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамблирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Голосование по максимуму(если хоть один говорит, что объект аномальный, значит объект будет аномальным)\n",
    "- Голосование большинством\n",
    "- Голосование взвешенное \n",
    "\n",
    "- Попробовать Random Subspaces\n",
    "- Попробовать разную векторизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    \n",
    "    def __init__(self, n_start, max_samples):\n",
    "        self.data = None\n",
    "        self.pos = 0\n",
    "        self.initialized = False\n",
    "        self.max_samples = max_samples\n",
    "        self.n_for_start = n_start\n",
    "        \n",
    "        if self.n_for_start > self.max_samples:\n",
    "            self.n_for_start = self.max_samples\n",
    "            print('n_start set to max_samples')\n",
    "            \n",
    "    def update_storage(self, X, start_count): # Warning: copying removed \n",
    "        \n",
    "        if self.data.shape[0] == self.max_samples:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count : ] # .copy()\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data[self.pos : ] = X[start_count : start_count + self.data.shape[0] - self.pos] # .copy()\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :] # .copy()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if self.pos + X.shape[0] - start_count <= self.max_samples:\n",
    "                self.data = np.concatenate((self.data, X[start_count : ]), axis=0)\n",
    "                self.pos += X.shape[0] - start_count\n",
    "\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data,\n",
    "                                            X[start_count : start_count + self.data.shape[0] - self.pos]), axis=0)\n",
    "                start_count += self.data.shape[0] - self.pos\n",
    "                self.pos = 0\n",
    "                self.data[self.pos : self.pos + X.shape[0] - start_count] = X[start_count :] # .copy()\n",
    "    \n",
    "    \n",
    "    def check_consistency(self, X):\n",
    "        if X.shape[0] > self.max_samples:\n",
    "            raise Exception('Too large batch for this model. Fix max_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate:\n",
    "    \n",
    "    def __init__(self, algorithms, n_start=100, max_samples = 1000):\n",
    "        self.storage = Storage(n_start, max_samples)\n",
    "        self.algorithms = algorithms\n",
    "        \n",
    "    \"\"\"\n",
    "    return: targets - list (N_algorithms, N_samples)\n",
    "    \"\"\"\n",
    "    def check_novelty(self, X):\n",
    "        \n",
    "        self.storage.check_consistency(X)\n",
    "        \n",
    "        start_count = 0\n",
    "        if not self.initialized:\n",
    "            X = np.asarray(X, dtype = X.dtype)\n",
    "            start_count = min( self.storage.n_for_start, X.shape[0])\n",
    "            self.storage.data = X[:start_count] # .copy()\n",
    "            self.storage.pos = start_count % self.storage.max_samples\n",
    "            self.storage.initialized = True\n",
    "        \n",
    "        targets = None\n",
    "        if start_count < X.shape[0]:\n",
    "            \n",
    "            self.storage.update_storage(X, start_count)\n",
    "            self.alg.fit(self.data)\n",
    "            \n",
    "            for alg in self.algorithms:\n",
    "                targets.append( [self.default_normal_score] * start_count + \n",
    "                                [target for target in alg.is_novelty(X[start_count:])] )\n",
    "            \n",
    "            # targets = decision_function(targets)\n",
    "            \n",
    "        else:\n",
    "            targets = [[self.default_normal_score] * start_count] * len(self.algorithms)\n",
    "        \n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vote():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def any_voting(self, targets):\n",
    "        return np.array(targets).any(axis=0)\n",
    "    \n",
    "    def max_voting(self, targets):\n",
    "        targets = np.array(targets)\n",
    "        novelty_count = np.sum(targets, axis=0)\n",
    "        normal_count = -(novelty_count - targets.shape[0])\n",
    "        return np.where(novelty_count > normal_count, 1, 0)\n",
    "    \n",
    "    # def weighted_voting(self, targets, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Один вид данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_data_ensemble(pipeline, batch_size, voting, repeat_count=1, verbose=1):\n",
    "    \"\"\"\n",
    "    pipeline[i]: [pp, vectorizer, svd]\n",
    "    \"\"\"\n",
    "\n",
    "    prec = []\n",
    "    acc = []\n",
    "    roc_auc = []\n",
    "    for iter_num in range(repeat_count):\n",
    "        answers = []\n",
    "        targets = []\n",
    "        for batch, target in tqdm( stream_generator(batch_size) ):\n",
    "            y_pred = []\n",
    "            for pl_num, (pp, vectorizer, svd, algorithms) in enumerate(pipeline):\n",
    "                batch = pp.preprocess1(batch)\n",
    "                vectors = vectorizer.vectorize(batch)\n",
    "                if svd is not None:\n",
    "                    vectors = svd.fit_transform(vectors) # don't use for doc-to-vec\n",
    "\n",
    "                for alg in algorithms:\n",
    "                    y_pred.append( list(alg.check_novelty(vectors)) )\n",
    "\n",
    "            answers += voting( y_pred )\n",
    "            targets += target\n",
    "\n",
    "        prec.append( precision_score(answers, targets) )\n",
    "        acc.append( accuracy_score(answers, targets) )\n",
    "        roc_auc.append( roc_auc_score(answers, targets) )\n",
    "        if verbose:\n",
    "            print('pipeline num: {}'.format(pl_num))\n",
    "            print( prec[-1] )\n",
    "            print( acc[-1] )\n",
    "            print( roc_auc[-1] )\n",
    "            print()\n",
    "    \n",
    "    if verbose:\n",
    "        print(np.mean(prec))\n",
    "        print(np.mean(acc))\n",
    "        print(np.mean(roc_auc))\n",
    "        \n",
    "    return np.mean(prec), np.mean(acc), np.mean(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_components = 10\n",
    "n_iter = 5\n",
    "vectorization = 'tf-idf'\n",
    "pp = Preprocessor()\n",
    "vectorizer = Vectorizer(vectorization, vocab_corpus)\n",
    "svd = TruncatedSVD(n_components=10, n_iter=5, random_state=42)\n",
    "scores_iter = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "for i in range(scores_iter):\n",
    "    algorithms = [\n",
    "        [],\n",
    "        [],\n",
    "        []\n",
    "#         Z_score(10)\n",
    "#         Dbscan(1, 0.5)\n",
    "#         LOF()\n",
    "#         SVM()\n",
    "    ]\n",
    "    \n",
    "    pipeline = [\n",
    "        [ Preprocessor(), Vectorizer(vectorization, vocab_corpus),\n",
    "            TruncatedSVD(n_components=5, n_iter=5, random_state=42), algorithms[0] ],\n",
    "        [ Preprocessor(), Vectorizer(vectorization, vocab_corpus),\n",
    "            TruncatedSVD(n_components=10, n_iter=5, random_state=5), algorithms[1] ],\n",
    "        [ Preprocessor(), Vectorizer(vectorization, vocab_corpus),\n",
    "            TruncatedSVD(n_components=20, n_iter=5, random_state=80), algorithms[2] ]\n",
    "    ]\n",
    "    # acc, f1, ra = \n",
    "    single_data_ensemble(pipeline=pipeline, batch_size=batch_size, voting=voting)\n",
    "\n",
    "#     acc_scores.append(acc)\n",
    "#     f1_scores.append(f1)\n",
    "#     roc_auc_scores.append(ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.mean(acc_scores) )\n",
    "print( np.mean(f1_scores) )\n",
    "print( np.mean(roc_auc_scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На будущее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- поправить предобработку и векторизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor.fit\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "\n",
    "https://towardsdatascience.com/anomaly-detection-with-local-outlier-factor-lof-d91e41df10f2\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/support-vector-machine-svm-for-anomaly-detection-73a8d676c331\n",
    "\n",
    "обкачать\n",
    "http://famouspoetsandpoems.com/poets/william_blake/poems/1192\n",
    "\n",
    "BERT для извлечения признаков:\n",
    "https://habr.com/ru/post/487358/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классная статья про метрики:\n",
    "\n",
    "https://habr.com/ru/company/ods/blog/328372/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно собрать также датасеты с разной тематикой и обучить нейронку находить различия тем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fizzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасеты twitter: https://data.world/datasets/twitter\n",
    "\n",
    "Датасеты twitter 2: https://www.trackmyhashtag.com/blog/free-twitter-datasets/\n",
    "\n",
    "Матрица из word2vec: https://coderoad.ru/51594165/Как-я-могу-извлечь-матрицы-WI-и-WO-из-gensim-word2vec\n",
    "\n",
    "Word2Vec хорошая статья: https://www.guru99.com/word-embedding-word2vec.html\n",
    "\n",
    "Statistical novelty detection: https://www.sciencedirect.com/science/article/pii/S0165168403002020?casa_token=S9hEVnhU2M8AAAAA:3Uq4_AAQi96h_iakEDa_NLrpZW-Jxa4k6Iawn-PNZ2Oe1o7BVaadTmw-4iyJRistQYYoST96\n",
    "\n",
    "Neuronet novelty detection: https://www.sciencedirect.com/science/article/pii/S0165168403002032?casa_token=-l0WADQMUzQAAAAA:-f4mPcs5SiIKjXMX2_xGyg_ouL-9p2P1p2ytc2B65QGQX5cJ9DDfG874pG64ljYL2Pd5WVqu\n",
    "\n",
    "A review of novelty detection: https://www.sciencedirect.com/science/article/pii/S016516841300515X?casa_token=1nbjiEGIeEcAAAAA:_DcczExMQg7c48FEN1CmoXj1xFnqwwEEvxEiavAQPnCpGr6ON0vatucpuSV8pnrhxZZeQb3c#bib125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
