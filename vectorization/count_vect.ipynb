{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Методы векторизации текста</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Частотное кодирование</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотное кодирование заключается в составлении вектора признаков по текстовому документу, каждому выделенному токену соответствует один признак, в качестве значения признака - количестве вхождений токена в документ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим класс CountVectorizer из библиотеки sklearn и исследуем его возможности. Сам класс преобразует коллекцию текстовых документов в матрицу количества токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all',remove=['headers','footers', 'quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = newsgroups['data'][0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препроцессинг текста перед токенизацией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ni am sure some bashers of pens fans are pretty confused about the lack\\nof any kind of posts about the recent pens massacre of the devils. actually,\\ni am  bit puzzled too and a bit relieved. however, i am going to put an end\\nto non-pittsburghers' relief with a bit of praise for the pens. man, they\\nare killing those devils worse than i thought. jagr just showed you why\\nhe is much better than his regular season stats. he is also a lot\\nfo fun to watch in the playoffs. bowman should let jagr have a lot of\\nfun in the next couple of games since the pens are going to beat the pulp out of jersey anyway. i was very disappointed not to see the islanders lose the final\\nregular season game.          pens rule!!!\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cv.build_preprocessor()(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация(удаляются слова длиной меньше 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'bashers',\n",
       " 'of',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'pens',\n",
       " 'massacre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'devils',\n",
       " 'actually',\n",
       " 'am',\n",
       " 'bit',\n",
       " 'puzzled',\n",
       " 'too',\n",
       " 'and',\n",
       " 'bit',\n",
       " 'relieved',\n",
       " 'however',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'put',\n",
       " 'an',\n",
       " 'end',\n",
       " 'to',\n",
       " 'non',\n",
       " 'pittsburghers',\n",
       " 'relief',\n",
       " 'with',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'praise',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pens',\n",
       " 'man',\n",
       " 'they',\n",
       " 'are',\n",
       " 'killing',\n",
       " 'those',\n",
       " 'devils',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'thought',\n",
       " 'jagr',\n",
       " 'just',\n",
       " 'showed',\n",
       " 'you',\n",
       " 'why',\n",
       " 'he',\n",
       " 'is',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'his',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " 'he',\n",
       " 'is',\n",
       " 'also',\n",
       " 'lot',\n",
       " 'fo',\n",
       " 'fun',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'in',\n",
       " 'the',\n",
       " 'playoffs',\n",
       " 'bowman',\n",
       " 'should',\n",
       " 'let',\n",
       " 'jagr',\n",
       " 'have',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'games',\n",
       " 'since',\n",
       " 'the',\n",
       " 'pens',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'the',\n",
       " 'pulp',\n",
       " 'out',\n",
       " 'of',\n",
       " 'jersey',\n",
       " 'anyway',\n",
       " 'was',\n",
       " 'very',\n",
       " 'disappointed',\n",
       " 'not',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'islanders',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'final',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'game',\n",
       " 'pens',\n",
       " 'rule']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cv.build_tokenizer()(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь \"обучим\" векторизацию на нашим данным, и посмотрим, что теперь может объект cv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_stop_words() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3a89b6016c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_stop_words() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токены(отсортированные лексикографически) теперь определяют отдельные признаки объекта \"документ\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'actually',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'anyway',\n",
       " 'are',\n",
       " 'bashers',\n",
       " 'beat',\n",
       " 'better',\n",
       " 'bit',\n",
       " 'bowman',\n",
       " 'confused',\n",
       " 'couple',\n",
       " 'devils',\n",
       " 'disappointed',\n",
       " 'end',\n",
       " 'fans',\n",
       " 'final',\n",
       " 'fo',\n",
       " 'for',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'games',\n",
       " 'going',\n",
       " 'have',\n",
       " 'he',\n",
       " 'his',\n",
       " 'however',\n",
       " 'in',\n",
       " 'is',\n",
       " 'islanders',\n",
       " 'jagr',\n",
       " 'jersey',\n",
       " 'just',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'lack',\n",
       " 'let',\n",
       " 'lose',\n",
       " 'lot',\n",
       " 'man',\n",
       " 'massacre',\n",
       " 'much',\n",
       " 'next',\n",
       " 'non',\n",
       " 'not',\n",
       " 'of',\n",
       " 'out',\n",
       " 'pens',\n",
       " 'pittsburghers',\n",
       " 'playoffs',\n",
       " 'posts',\n",
       " 'praise',\n",
       " 'pretty',\n",
       " 'pulp',\n",
       " 'put',\n",
       " 'puzzled',\n",
       " 'recent',\n",
       " 'regular',\n",
       " 'relief',\n",
       " 'relieved',\n",
       " 'rule',\n",
       " 'season',\n",
       " 'see',\n",
       " 'should',\n",
       " 'showed',\n",
       " 'since',\n",
       " 'some',\n",
       " 'stats',\n",
       " 'sure',\n",
       " 'than',\n",
       " 'the',\n",
       " 'they',\n",
       " 'those',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'too',\n",
       " 'very',\n",
       " 'was',\n",
       " 'watch',\n",
       " 'why',\n",
       " 'with',\n",
       " 'worse',\n",
       " 'you']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сами векторы признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['I very very glad to see you here, writing the text vectorization methods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 2 1 1]]\n",
      "['glad', 'here', 'methods', 'see', 'text', 'the', 'to', 'vectorization', 'very', 'writing', 'you']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "print(cv.fit_transform(data).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, признаки соответствуют количеству вхождений в документ токенов из отсортированного списка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь векторизовать текст, не содержащий общих слов с первый документом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [\"No, i don't wanna sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0]]\n",
      "['glad', 'here', 'methods', 'see', 'text', 'the', 'to', 'vectorization', 'very', 'writing', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(cv.transform(data2).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логично, что с словаре, составленным по первому документу, 0 совпадающих признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ограничить размер словаря(количество признаков), тогда в векторизации будут использоваться самые частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['first second first three first four five five six']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2 1]]\n",
      "['first', 'five', 'four']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=3)\n",
    "print(cv.fit_transform(data).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно задавать свой словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]]\n",
      "['five', 'six', 'three']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(vocabulary=['five', 'six', 'three'], max_features=4)\n",
    "print(cv.fit_transform(data).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно менять шаблон, согласно которому мы отбираем токены при токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['<div>ну так вот:<p><a>текст<br><a><p><div>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2 2]]\n",
      "['<a>', '<br>', '<div>', '<p>']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(token_pattern=r'<\\w*>')\n",
    "print(cv.fit_transform(data).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А также добавлять стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['I very very glad to see you here, writing the text vectorization methods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 2 1 1]]\n",
      "['glad', 'here', 'methods', 'see', 'text', 'vectorization', 'very', 'writing', 'you']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=frozenset(['to', 'the']))\n",
    "print(cv.fit_transform(data).toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>One-hot кодирование</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем признак, соответствующий токену, равным единице, если слово есть в словаре, иначе 0. Осуществляется также через CountVectorizer с помощью добавление флага <i>binary</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['I very very glad to see you here, writing the text vectorization methods']\n",
    "data2 = ['I not very glad to write this message, do you?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glad', 'here', 'methods', 'see', 'text', 'the', 'to', 'vectorization', 'very', 'writing', 'you']\n",
      "[[1 1 1 1 1 1 1 1 1 1 1]]\n",
      "[[1 0 0 0 0 0 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(data)\n",
    "print(cv.get_feature_names())\n",
    "print(cv.transform(data).toarray())\n",
    "print(cv.transform(data2).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, доступна вся функциональность CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Применение на больших текстовых данных</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим поведение векторизации на текстовых данных(роман \"Идиот\" Ф.М.Достоевского). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('idiot.txt', 'r')\n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 429 ms, sys: 7.15 ms, total: 437 ms\n",
      "Wall time: 440 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['аглая',\n",
       " 'без',\n",
       " 'бы',\n",
       " 'был',\n",
       " 'была',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'во',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'время']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(max_features=100)\n",
    "cv.fit(data)\n",
    "cv.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9549, 100)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = cv.transform(data).toarray()\n",
    "vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 22.2 ms, total: 418 ms\n",
      "Wall time: 425 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['аглая',\n",
       " 'без',\n",
       " 'бы',\n",
       " 'был',\n",
       " 'была',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'во',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'время']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(max_features=100, binary=True)\n",
    "cv.fit(data)\n",
    "cv.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9549, 100)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = cv.transform(data).toarray()\n",
    "vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае произведение у нас является корпусом, а параграфы в нем - документами. В коде мы составляет словарь из токенов со всего текста, а дальше векторизуем параграфы(оставляя только 100 самых приоритетных признаков). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы рассмотрели как можно осуществлять частотное и one-hot кодирование с помощью класса такого CountVectorizer библиотеки sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
